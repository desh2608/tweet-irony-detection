{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "import re\n",
    "import os\n",
    "import wordsegment as ws\n",
    "import preprocessor as p\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from keras.layers import Embedding, Dense, Input, MaxPooling2D, Dropout, LSTM, Bidirectional, Reshape\n",
    "from keras.models import Model,Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras import initializers, optimizers\n",
    "from keras import backend as K\n",
    "\n",
    "os.environ['KERAS_BACKEND']='tensorflow'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./datasets/train/SemEval2018-T3-train-taskA.txt\", sep=\"\\t\")\n",
    "ws.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hashtags(tweet):\n",
    "    parsed_tweet = p.parse(tweet.lower())\n",
    "    parsed_hashtags = parsed_tweet.hashtags\n",
    "    all_hashtags = {}\n",
    "    \n",
    "    hashtags = []\n",
    "    if parsed_hashtags is not None:\n",
    "        for h in parsed_hashtags:\n",
    "            temp = h.match[1:].lower()\n",
    "            hashtag = \" \".join(ws.segment(temp))\n",
    "            if hashtag in all_hashtags:\n",
    "                all_hashtags[hashtag] += 1\n",
    "            else:\n",
    "                all_hashtags[hashtag] = 1\n",
    "            hashtags.append(hashtag)\n",
    "\n",
    "    hashtags_str = (\" \").join(hashtags)\n",
    "    return hashtags_str, len(hashtags), all_hashtags\n",
    "\n",
    "def get_text(tweet):\n",
    "    clean_tweet = p.clean(tweet)\n",
    "    clean_tweet = re.sub(r'[^\\w\\s]','',clean_tweet)\n",
    "    return clean_tweet.lower()\n",
    "\n",
    "\n",
    "def get_emotion(tweet):\n",
    "    emotion_keys = {}\n",
    "    result = re.findall(r\":\\w+_\\w+:\",tweet)\n",
    "    if result is not None:\n",
    "        emotions = []\n",
    "        for i in range(len(result)):\n",
    "            emotion = result[i][1:-1]\n",
    "            emotions.append(emotion)\n",
    "            if emotion in emotion_keys:\n",
    "                emotion_keys[emotion] += 1\n",
    "            else:\n",
    "                emotion_keys[emotion] = 1\n",
    "    return emotions, emotion_keys "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet index</th>\n",
       "      <th>Label</th>\n",
       "      <th>Tweet text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>length</th>\n",
       "      <th>hashtag_dict</th>\n",
       "      <th>tweet</th>\n",
       "      <th>emotion</th>\n",
       "      <th>emotion_dict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Sweet United Nations video. Just in time for Christmas. #imagine #NoReligion  http://t.co/fej2v3OUBR</td>\n",
       "      <td>imagine no religion</td>\n",
       "      <td>2</td>\n",
       "      <td>{'no religion': 1, 'imagine': 1}</td>\n",
       "      <td>sweet united nations video just in time for christmas</td>\n",
       "      <td>[]</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>@mrdahl87 We are rumored to have talked to Erv's agent... and the Angels asked about Ed Escobar... that's hardly nothing    ;)</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "      <td>we are rumored to have talked to ervs agent and the angels asked about ed escobar thats hardly nothing</td>\n",
       "      <td>[]</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Hey there! Nice to see you Minnesota/ND Winter Weather</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "      <td>hey there nice to see you minnesotand winter weather</td>\n",
       "      <td>[]</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3 episodes left I'm dying over here</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "      <td>3 episodes left im dying over here</td>\n",
       "      <td>[]</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>I can't breathe! was chosen as the most notable quote of the year in an annual list released by a Yale University librarian</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "      <td>i cant breathe was chosen as the most notable quote of the year in an annual list released by a yale university librarian</td>\n",
       "      <td>[]</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Tweet index  Label  \\\n",
       "0  1            1       \n",
       "1  2            1       \n",
       "2  3            1       \n",
       "3  4            0       \n",
       "4  5            1       \n",
       "\n",
       "                                                                                                                       Tweet text  \\\n",
       "0  Sweet United Nations video. Just in time for Christmas. #imagine #NoReligion  http://t.co/fej2v3OUBR                             \n",
       "1  @mrdahl87 We are rumored to have talked to Erv's agent... and the Angels asked about Ed Escobar... that's hardly nothing    ;)   \n",
       "2  Hey there! Nice to see you Minnesota/ND Winter Weather                                                                           \n",
       "3  3 episodes left I'm dying over here                                                                                              \n",
       "4  I can't breathe! was chosen as the most notable quote of the year in an annual list released by a Yale University librarian      \n",
       "\n",
       "              hashtags  length                      hashtag_dict  \\\n",
       "0  imagine no religion  2       {'no religion': 1, 'imagine': 1}   \n",
       "1                       0       {}                                 \n",
       "2                       0       {}                                 \n",
       "3                       0       {}                                 \n",
       "4                       0       {}                                 \n",
       "\n",
       "                                                                                                                       tweet  \\\n",
       "0  sweet united nations video just in time for christmas                                                                       \n",
       "1  we are rumored to have talked to ervs agent and the angels asked about ed escobar thats hardly nothing                      \n",
       "2  hey there nice to see you minnesotand winter weather                                                                        \n",
       "3  3 episodes left im dying over here                                                                                          \n",
       "4  i cant breathe was chosen as the most notable quote of the year in an annual list released by a yale university librarian   \n",
       "\n",
       "  emotion emotion_dict  \n",
       "0  []      {}           \n",
       "1  []      {}           \n",
       "2  []      {}           \n",
       "3  []      {}           \n",
       "4  []      {}           "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['hashtags'], data['length'], data['hashtag_dict'] = zip(*data['Tweet text'].map(get_hashtags)) \n",
    "data[\"tweet\"] = data['Tweet text'].map(get_text)\n",
    "data['emotion'], data['emotion_dict'] = zip(*data['tweet'].map(get_emotion))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = ''\n",
    "GLOVE_DIR = os.path.join(BASE_DIR, 'glove')\n",
    "TEXT_LENGTH = max(len(x.split(' ')) for x in data['tweet'].tolist())\n",
    "HASHTAG_LENGTH = max(len(x.split(' ')) for x in data['hashtags'].tolist())\n",
    "MAX_NUM_WORDS = 20000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2\n",
    "NUM_CLASSES = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors.\n",
      "Found 1193514 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# first, build index mapping words in the embeddings set\n",
    "# to their embedding vector\n",
    "import io\n",
    "\n",
    "print('Indexing word vectors.')\n",
    "\n",
    "embeddings_index = {}\n",
    "f = io.open(os.path.join(GLOVE_DIR, 'glove.twitter.27B.100d.txt'),encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8562 unique tokens.\n",
      "Found 2833 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "# vectorize the text samples into a 2D integer tensor\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "def vectorize_data(text, MAX_NUM_WORDS, MAX_SEQUENCE_LENGTH):\n",
    "    tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "    tokenizer.fit_on_texts(text)\n",
    "    sequences = tokenizer.texts_to_sequences(text)\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    print('Found %s unique tokens.' % len(word_index))\n",
    "    \n",
    "    for word,idx in word_index.items():\n",
    "        word_index[word] = idx - 1\n",
    "\n",
    "    data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    return data, word_index\n",
    "\n",
    "labels = to_categorical(np.asarray(data['Label']))\n",
    "x_tweet, tweet_token_index = vectorize_data(data['tweet'],MAX_NUM_WORDS,TEXT_LENGTH)\n",
    "x_hashtags, ht_token_index = vectorize_data(data['hashtags'],MAX_NUM_WORDS,HASHTAG_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into a training set and a validation set\n",
    "indices = np.arange(x_tweet.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "x_tweet = x_tweet[indices]\n",
    "x_hashtags = x_hashtags[indices]\n",
    "labels = labels[indices]\n",
    "num_validation_samples = int(VALIDATION_SPLIT * x_tweet.shape[0])\n",
    "\n",
    "x_tweet_train = x_tweet[:-num_validation_samples]\n",
    "x_hashtags_train = x_hashtags[:-num_validation_samples]\n",
    "y_train = labels[:-num_validation_samples]\n",
    "x_tweet_val = x_tweet[-num_validation_samples:]\n",
    "x_hashtags_val = x_hashtags[-num_validation_samples:]\n",
    "y_val = labels[-num_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "with np.load('./datasets/taskA-punctuation.npz') as d:\n",
    "    w_hash = d['num_hash']\n",
    "    w_at = d['num_at']\n",
    "    w_exc = d['num_exclaim']\n",
    "    \n",
    "W_hash_tr = w_hash[:-num_validation_samples]\n",
    "W_hash_dev = w_hash[-num_validation_samples:]\n",
    "W_at_tr = w_at[:-num_validation_samples]\n",
    "W_at_dev = w_at[-num_validation_samples:]\n",
    "W_exc_tr = w_exc[:-num_validation_samples]\n",
    "W_exc_dev = w_exc[-num_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare embedding matrix\n",
    "def get_embedding_matrix(word_index):\n",
    "    num_words = min(MAX_NUM_WORDS, len(word_index)+1)\n",
    "    embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= MAX_NUM_WORDS:\n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i-1] = embedding_vector\n",
    "    return embedding_matrix\n",
    "\n",
    "tweet_emb = get_embedding_matrix(tweet_token_index)\n",
    "hashtag_emb = get_embedding_matrix(ht_token_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving/loading preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blstm_maxpool(x, word_index, emb_matrix, num_filters, max_seq_len, learn_rate):\n",
    "    num_words = min(MAX_NUM_WORDS, len(word_index)+1)\n",
    "    y = Embedding(num_words,EMBEDDING_DIM,weights=[emb_matrix],\n",
    "                                        input_length=max_seq_len,trainable=False)(x)\n",
    "    y = Bidirectional(LSTM(num_filters, return_sequences=True))(y)\n",
    "    y = Reshape((max_seq_len,2*num_filters,1))(y)\n",
    "    y = MaxPooling2D(pool_size=(max_seq_len,1), strides=None, padding='valid')(y)\n",
    "    y = Reshape((2*num_filters,))(y)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_FILTERS = 50\n",
    "LEARNING_RATE = 0.005\n",
    "tweet = Input(shape=(TEXT_LENGTH,), dtype='int32')\n",
    "hashtag = Input(shape=(HASHTAG_LENGTH,), dtype='int32')\n",
    "\n",
    "tweet_lstm_vec = blstm_maxpool(tweet,tweet_token_index,tweet_emb, NUM_FILTERS, TEXT_LENGTH, LEARNING_RATE)\n",
    "ht_lstm_vec = blstm_maxpool(hashtag,ht_token_index,hashtag_emb, NUM_FILTERS, HASHTAG_LENGTH, LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def holographic_merge(inp):\n",
    "    [a, b] = inp\n",
    "    a_fft = tf.fft(tf.complex(a, 0.0))\n",
    "    b_fft = tf.fft(tf.complex(b, 0.0))\n",
    "    ifft = tf.ifft(tf.conj(a_fft) * b_fft)\n",
    "    return tf.cast(tf.real(ifft), 'float32') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Lambda\n",
    "\n",
    "h_circ = Lambda(holographic_merge)([tweet_lstm_vec,ht_lstm_vec])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropout and dense layer\n",
    "\n",
    "h_circ = Dropout(0.3)(h_circ)\n",
    "preds = Dense(NUM_CLASSES, activation='softmax')(h_circ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model([tweet,hashtag],preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 172)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_12 (InputLayer)           (None, 23)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_10 (Embedding)        (None, 172, 100)     856300      input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_11 (Embedding)        (None, 23, 100)      283400      input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_9 (Bidirectional) (None, 172, 100)     60400       embedding_10[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_10 (Bidirectional (None, 23, 100)      60400       embedding_11[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reshape_17 (Reshape)            (None, 172, 100, 1)  0           bidirectional_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "reshape_19 (Reshape)            (None, 23, 100, 1)   0           bidirectional_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2D)  (None, 1, 100, 1)    0           reshape_17[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling2D) (None, 1, 100, 1)    0           reshape_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape_18 (Reshape)            (None, 100)          0           max_pooling2d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "reshape_20 (Reshape)            (None, 100)          0           max_pooling2d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, 100)          0           reshape_18[0][0]                 \n",
      "                                                                 reshape_20[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 100)          0           lambda_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 2)            202         dropout_5[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,260,702\n",
      "Trainable params: 121,002\n",
      "Non-trainable params: 1,139,700\n",
      "__________________________________________________________________________________________________\n",
      "Training\n",
      "Epoch 1/10\n",
      "3054/3054 [==============================] - 61s 20ms/step - loss: 0.7707 - acc: 0.4921\n",
      "Epoch 2/10\n",
      "3054/3054 [==============================] - 43s 14ms/step - loss: 0.6768 - acc: 0.5576\n",
      "Epoch 3/10\n",
      "3054/3054 [==============================] - 40s 13ms/step - loss: 0.6497 - acc: 0.6189\n",
      "Epoch 4/10\n",
      "3054/3054 [==============================] - 40s 13ms/step - loss: 0.6221 - acc: 0.6526\n",
      "Epoch 5/10\n",
      "3054/3054 [==============================] - 41s 13ms/step - loss: 0.5883 - acc: 0.6883\n",
      "Epoch 6/10\n",
      "3054/3054 [==============================] - 42s 14ms/step - loss: 0.5238 - acc: 0.7384\n",
      "Epoch 7/10\n",
      "3054/3054 [==============================] - 48s 16ms/step - loss: 0.4735 - acc: 0.7764\n",
      "Epoch 8/10\n",
      "3054/3054 [==============================] - 47s 15ms/step - loss: 0.4130 - acc: 0.8153\n",
      "Epoch 9/10\n",
      "3054/3054 [==============================] - 46s 15ms/step - loss: 0.3322 - acc: 0.8595\n",
      "Epoch 10/10\n",
      "3054/3054 [==============================] - 49s 16ms/step - loss: 0.2915 - acc: 0.8831\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2641e87f2e8>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 20\n",
    "EPOCHS = 10\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "print('Training')\n",
    "model.fit([x_tweet_train, x_hashtags_train], y_train,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=EPOCHS,\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving/Loading the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving\n",
    "model.save('./keras_models/model1.h5')\n",
    "\n",
    "## Loading\n",
    "# from keras.models import load_model\n",
    "# model = load_model('./keras_models/model1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: [ 0.57305936  0.64      ]\n",
      "\n",
      "Recall: [ 0.68206522  0.52658228]\n",
      "\n",
      "f1_score: [ 0.62282878  0.57777778]\n",
      "\n",
      "[[251 117]\n",
      " [187 208]]\n",
      ":: Classification Report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.5731    0.6821    0.6228       368\n",
      "          1     0.6400    0.5266    0.5778       395\n",
      "\n",
      "avg / total     0.6077    0.6016    0.5995       763\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temp = model.predict([x_tweet_val,x_hashtags_val])\n",
    "y_pred  = np.argmax(temp, 1)\n",
    "y_true = np.argmax(y_val, 1)\n",
    "precision = metrics.precision_score(y_true, y_pred, average=None)\n",
    "recall = metrics.recall_score(y_true, y_pred, average=None)\n",
    "f1_score = metrics.f1_score(y_true, y_pred, average=None)\n",
    "print(\"Precision: \" + str(precision) + \"\\n\")\n",
    "print(\"Recall: \" + str(recall) + \"\\n\")\n",
    "print(\"f1_score: \" + str(f1_score) + \"\\n\")\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "print(\":: Classification Report\")\n",
    "print(classification_report(y_true, y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_holographic_output(x_tweet, x_hashtags):\n",
    "    intermediate_layer_model = Model(inputs=model.input, outputs=model.get_layer('lambda_5').output)\n",
    "    intermediate_output = intermediate_layer_model.predict([x_tweet, x_hashtags])\n",
    "    return np.array(list(intermediate_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all the input arrays must have same number of dimensions",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-86-f613cddcba86>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m X_train_xgb = np.concatenate((get_holographic_output(x_tweet_train, x_hashtags_train),\n\u001b[1;32m----> 2\u001b[1;33m                               W_hash_tr,W_at_tr,W_exc_tr),axis=1)\n\u001b[0m\u001b[0;32m      3\u001b[0m X_val_xgb = np.concatenate((get_holographic_output(x_tweet_val, x_hashtags_val),\n\u001b[0;32m      4\u001b[0m                             W_hash_dev,W_at_dev,W_exc_dev),axis=1)\n\u001b[0;32m      5\u001b[0m \u001b[0my_train_xgb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: all the input arrays must have same number of dimensions"
     ]
    }
   ],
   "source": [
    "X_train_xgb = np.concatenate((get_holographic_output(x_tweet_train, x_hashtags_train),\n",
    "                              W_hash_tr,W_at_tr,W_exc_tr),axis=1)\n",
    "X_val_xgb = np.concatenate((get_holographic_output(x_tweet_val, x_hashtags_val),\n",
    "                            W_hash_dev,W_at_dev,W_exc_dev),axis=1)\n",
    "y_train_xgb = np.argmax(y_train, axis=1)\n",
    "y_val_xgb = np.argmax(y_val, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3071,)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: [ 0.584       0.61597938]\n",
      "\n",
      "Recall: [ 0.5951087   0.60506329]\n",
      "\n",
      "f1_score: [ 0.58950202  0.61047254]\n",
      "\n",
      "[[219 149]\n",
      " [156 239]]\n",
      ":: Classification Report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.5840    0.5951    0.5895       368\n",
      "          1     0.6160    0.6051    0.6105       395\n",
      "\n",
      "avg / total     0.6006    0.6003    0.6004       763\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "logreg = xgb.XGBClassifier(max_depth=3, learning_rate=0.1, n_estimators=100, silent=False, \n",
    "                           objective='binary:logistic', booster='gbtree', n_jobs=1, nthread=None, \n",
    "                           gamma=0, min_child_weight=1, max_delta_step=0, subsample=1, colsample_bytree=1, \n",
    "                           colsample_bylevel=1, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, base_score=0.5, \n",
    "                           random_state=0, seed=None, missing=None)\n",
    "logreg.fit(X_train_xgb, y_train_xgb)\n",
    "\n",
    "y_pred = logreg.predict(X_val_xgb)\n",
    "y_true = y_val_xgb\n",
    "precision = metrics.precision_score(y_true, y_pred, average=None)\n",
    "recall = metrics.recall_score(y_true, y_pred, average=None)\n",
    "f1_score = metrics.f1_score(y_true, y_pred, average=None)\n",
    "print(\"Precision: \" + str(precision) + \"\\n\")\n",
    "print(\"Recall: \" + str(recall) + \"\\n\")\n",
    "print(\"f1_score: \" + str(f1_score) + \"\\n\")\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "print(\":: Classification Report\")\n",
    "print(classification_report(y_true, y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
