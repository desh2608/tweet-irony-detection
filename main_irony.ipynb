{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "import re\n",
    "import os\n",
    "import wordsegment as ws\n",
    "import preprocessor as p\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from keras.layers import Embedding, Dense, Input, MaxPooling2D, Dropout, LSTM, Bidirectional, Reshape\n",
    "from keras.models import Model,Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras import initializers, optimizers\n",
    "from keras import backend as K\n",
    "\n",
    "os.environ['KERAS_BACKEND']='tensorflow'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./datasets/train/SemEval2018-T3-train-taskA_emoji.txt\", sep=\"\\t\")\n",
    "data = data[data['Tweet text'].map(len)<=140]\n",
    "ws.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hashtags(tweet):\n",
    "    parsed_tweet = p.parse(tweet.lower())\n",
    "    parsed_hashtags = parsed_tweet.hashtags\n",
    "    all_hashtags = {}\n",
    "    \n",
    "    hashtags = []\n",
    "    if parsed_hashtags is not None:\n",
    "        for h in parsed_hashtags:\n",
    "            temp = h.match[1:].lower()\n",
    "            hashtag = \" \".join(ws.segment(temp))\n",
    "            if hashtag in all_hashtags:\n",
    "                all_hashtags[hashtag] += 1\n",
    "            else:\n",
    "                all_hashtags[hashtag] = 1\n",
    "            hashtags.append(hashtag)\n",
    "\n",
    "    hashtags_str = (\" \").join(hashtags)\n",
    "    return hashtags_str, len(hashtags), all_hashtags\n",
    "\n",
    "def get_text(tweet):\n",
    "    clean_tweet = p.clean(tweet)\n",
    "    clean_tweet = re.sub(r'[^\\w\\s]','',clean_tweet)\n",
    "    return clean_tweet.lower()\n",
    "\n",
    "\n",
    "def get_emotion(tweet):\n",
    "    emotion_keys = {}\n",
    "    result = re.findall(r\":\\w+_\\w+:\",tweet)\n",
    "    if result is not None:\n",
    "        emotions = []\n",
    "        for i in range(len(result)):\n",
    "            emotion = result[i][1:-1]\n",
    "            emotions.append(emotion)\n",
    "            if emotion in emotion_keys:\n",
    "                emotion_keys[emotion] += 1\n",
    "            else:\n",
    "                emotion_keys[emotion] = 1\n",
    "    return emotions, emotion_keys "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet index</th>\n",
       "      <th>Label</th>\n",
       "      <th>Tweet text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>length</th>\n",
       "      <th>hashtag_dict</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Sweet United Nations video. Just in time for Christmas. #imagine #NoReligion  http://t.co/fej2v3OUBR</td>\n",
       "      <td>imagine no religion</td>\n",
       "      <td>2</td>\n",
       "      <td>{'imagine': 1, 'no religion': 1}</td>\n",
       "      <td>sweet united nations video just in time for christmas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>@mrdahl87 We are rumored to have talked to Erv's agent... and the Angels asked about Ed Escobar... that's hardly nothing    ;)</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "      <td>we are rumored to have talked to ervs agent and the angels asked about ed escobar thats hardly nothing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Hey there! Nice to see you Minnesota/ND Winter Weather</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "      <td>hey there nice to see you minnesotand winter weather</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3 episodes left I'm dying over here</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "      <td>3 episodes left im dying over here</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>I can't breathe! was chosen as the most notable quote of the year in an annual list released by a Yale University librarian</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "      <td>i cant breathe was chosen as the most notable quote of the year in an annual list released by a yale university librarian</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Tweet index  Label  \\\n",
       "0  1            1       \n",
       "1  2            1       \n",
       "2  3            1       \n",
       "3  4            0       \n",
       "4  5            1       \n",
       "\n",
       "                                                                                                                       Tweet text  \\\n",
       "0  Sweet United Nations video. Just in time for Christmas. #imagine #NoReligion  http://t.co/fej2v3OUBR                             \n",
       "1  @mrdahl87 We are rumored to have talked to Erv's agent... and the Angels asked about Ed Escobar... that's hardly nothing    ;)   \n",
       "2  Hey there! Nice to see you Minnesota/ND Winter Weather                                                                           \n",
       "3  3 episodes left I'm dying over here                                                                                              \n",
       "4  I can't breathe! was chosen as the most notable quote of the year in an annual list released by a Yale University librarian      \n",
       "\n",
       "              hashtags  length                      hashtag_dict  \\\n",
       "0  imagine no religion  2       {'imagine': 1, 'no religion': 1}   \n",
       "1                       0       {}                                 \n",
       "2                       0       {}                                 \n",
       "3                       0       {}                                 \n",
       "4                       0       {}                                 \n",
       "\n",
       "                                                                                                                       tweet  \n",
       "0  sweet united nations video just in time for christmas                                                                      \n",
       "1  we are rumored to have talked to ervs agent and the angels asked about ed escobar thats hardly nothing                     \n",
       "2  hey there nice to see you minnesotand winter weather                                                                       \n",
       "3  3 episodes left im dying over here                                                                                         \n",
       "4  i cant breathe was chosen as the most notable quote of the year in an annual list released by a yale university librarian  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['hashtags'], data['length'], data['hashtag_dict'] = zip(*data['Tweet text'].map(get_hashtags)) \n",
    "data[\"tweet\"] = data['Tweet text'].map(get_text)\n",
    "# data['emotion'], data['emotion_dict'] = zip(*data['tweet'].map(get_emotion))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = ''\n",
    "GLOVE_DIR = os.path.join(BASE_DIR, 'glove')\n",
    "TEXT_LENGTH = max(len(x.split(' ')) for x in data['tweet'].tolist())\n",
    "HASHTAG_LENGTH = max(len(x.split(' ')) for x in data['hashtags'].tolist())\n",
    "MAX_NUM_WORDS = 20000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2\n",
    "NUM_CLASSES = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors.\n",
      "Found 1193514 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# first, build index mapping words in the embeddings set\n",
    "# to their embedding vector\n",
    "import io\n",
    "\n",
    "print('Indexing word vectors.')\n",
    "\n",
    "embeddings_index = {}\n",
    "f = io.open(os.path.join(GLOVE_DIR, 'glove.twitter.27B.100d.txt'),encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8225 unique tokens.\n",
      "Found 2824 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "# vectorize the text samples into a 2D integer tensor\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "def vectorize_data(text, MAX_NUM_WORDS, MAX_SEQUENCE_LENGTH):\n",
    "    tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "    tokenizer.fit_on_texts(text)\n",
    "    sequences = tokenizer.texts_to_sequences(text)\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    print('Found %s unique tokens.' % len(word_index))\n",
    "    \n",
    "    for word,idx in word_index.items():\n",
    "        word_index[word] = idx - 1\n",
    "\n",
    "    data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    return data, word_index\n",
    "\n",
    "labels = to_categorical(np.asarray(data['Label']))\n",
    "x_tweet, tweet_token_index = vectorize_data(data['tweet'],MAX_NUM_WORDS,TEXT_LENGTH)\n",
    "x_hashtags, ht_token_index = vectorize_data(data['hashtags'],MAX_NUM_WORDS,HASHTAG_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into a training set and a validation set\n",
    "indices = np.arange(x_tweet.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "x_tweet = x_tweet[indices]\n",
    "x_hashtags = x_hashtags[indices]\n",
    "labels = labels[indices]\n",
    "num_validation_samples = int(VALIDATION_SPLIT * x_tweet.shape[0])\n",
    "\n",
    "x_tweet_train = x_tweet[:-num_validation_samples]\n",
    "x_hashtags_train = x_hashtags[:-num_validation_samples]\n",
    "y_train = labels[:-num_validation_samples]\n",
    "x_tweet_val = x_tweet[-num_validation_samples:]\n",
    "x_hashtags_val = x_hashtags[-num_validation_samples:]\n",
    "y_val = labels[-num_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with np.load('./datasets/taskA-punctuation.npz') as d:\n",
    "#     w_hash = d['num_hash']\n",
    "#     w_at = d['num_at']\n",
    "#     w_exc = d['num_exclaim']\n",
    "    \n",
    "# W_hash_tr = w_hash[:-num_validation_samples]\n",
    "# W_hash_dev = w_hash[-num_validation_samples:]\n",
    "# W_at_tr = w_at[:-num_validation_samples]\n",
    "# W_at_dev = w_at[-num_validation_samples:]\n",
    "# W_exc_tr = w_exc[:-num_validation_samples]\n",
    "# W_exc_dev = w_exc[-num_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare embedding matrix\n",
    "def get_embedding_matrix(word_index):\n",
    "    num_words = min(MAX_NUM_WORDS, len(word_index)+1)\n",
    "    embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= MAX_NUM_WORDS:\n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i-1] = embedding_vector\n",
    "    return embedding_matrix\n",
    "\n",
    "tweet_emb = get_embedding_matrix(tweet_token_index)\n",
    "hashtag_emb = get_embedding_matrix(ht_token_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving/loading preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blstm_maxpool(x, word_index, emb_matrix, num_filters, max_seq_len, learn_rate):\n",
    "    num_words = min(MAX_NUM_WORDS, len(word_index)+1)\n",
    "    y = Embedding(num_words,EMBEDDING_DIM,weights=[emb_matrix],\n",
    "                                        input_length=max_seq_len,trainable=False)(x)\n",
    "    y = Bidirectional(LSTM(num_filters, return_sequences=True))(y)\n",
    "    y = Reshape((max_seq_len,2*num_filters,1))(y)\n",
    "    y = MaxPooling2D(pool_size=(max_seq_len,1), strides=None, padding='valid')(y)\n",
    "    y = Reshape((2*num_filters,))(y)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_FILTERS = 100\n",
    "LEARNING_RATE = 0.005\n",
    "tweet = Input(shape=(TEXT_LENGTH,), dtype='int32')\n",
    "hashtag = Input(shape=(HASHTAG_LENGTH,), dtype='int32')\n",
    "\n",
    "tweet_lstm_vec = blstm_maxpool(tweet,tweet_token_index,tweet_emb, NUM_FILTERS, TEXT_LENGTH, LEARNING_RATE)\n",
    "ht_lstm_vec = blstm_maxpool(hashtag,ht_token_index,hashtag_emb, NUM_FILTERS, HASHTAG_LENGTH, LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def holographic_merge(inp):\n",
    "    [a, b] = inp\n",
    "    a_fft = tf.fft(tf.complex(a, 0.0))\n",
    "    b_fft = tf.fft(tf.complex(b, 0.0))\n",
    "    ifft = tf.ifft(tf.conj(a_fft) * b_fft)\n",
    "    return tf.cast(tf.real(ifft), 'float32') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Lambda\n",
    "\n",
    "h_circ = Lambda(holographic_merge)([tweet_lstm_vec,ht_lstm_vec])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropout and dense layer\n",
    "\n",
    "h_circ = Dropout(0.3)(h_circ)\n",
    "preds = Dense(NUM_CLASSES, activation='softmax')(h_circ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model([tweet,hashtag],preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 32)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 23)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 32, 100)      822600      input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 23, 100)      282500      input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 32, 200)      160800      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 23, 200)      160800      embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 32, 200, 1)   0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)             (None, 23, 200, 1)   0           bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 1, 200, 1)    0           reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 1, 200, 1)    0           reshape_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 200)          0           max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "reshape_4 (Reshape)             (None, 200)          0           max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 200)          0           reshape_2[0][0]                  \n",
      "                                                                 reshape_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 200)          0           lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 2)            402         dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,427,102\n",
      "Trainable params: 322,002\n",
      "Non-trainable params: 1,105,100\n",
      "__________________________________________________________________________________________________\n",
      "Training\n",
      "Epoch 1/10\n",
      "3051/3051 [==============================] - 33s 11ms/step - loss: 0.7519 - acc: 0.5143\n",
      "Epoch 2/10\n",
      "3051/3051 [==============================] - 20s 7ms/step - loss: 0.6678 - acc: 0.5782\n",
      "Epoch 3/10\n",
      "3051/3051 [==============================] - 19s 6ms/step - loss: 0.6331 - acc: 0.6408\n",
      "Epoch 4/10\n",
      "3051/3051 [==============================] - 19s 6ms/step - loss: 0.5971 - acc: 0.6844\n",
      "Epoch 5/10\n",
      "3051/3051 [==============================] - 20s 6ms/step - loss: 0.5394 - acc: 0.7260\n",
      "Epoch 6/10\n",
      "3051/3051 [==============================] - 22s 7ms/step - loss: 0.4673 - acc: 0.7811\n",
      "Epoch 7/10\n",
      "3051/3051 [==============================] - 22s 7ms/step - loss: 0.3553 - acc: 0.8482\n",
      "Epoch 8/10\n",
      "3051/3051 [==============================] - 22s 7ms/step - loss: 0.2697 - acc: 0.8938\n",
      "Epoch 9/10\n",
      "3051/3051 [==============================] - 22s 7ms/step - loss: 0.1823 - acc: 0.9358\n",
      "Epoch 10/10\n",
      "3051/3051 [==============================] - 21s 7ms/step - loss: 0.1063 - acc: 0.9669\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2b37b4b3f28>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 20\n",
    "EPOCHS = 10\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "print('Training')\n",
    "model.fit([x_tweet_train, x_hashtags_train], y_train,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=EPOCHS,\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving/Loading the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Saving\n",
    "# model.save('./keras_models/model1.h5')\n",
    "\n",
    "# ## Loading\n",
    "# # from keras.models import load_model\n",
    "# # model = load_model('./keras_models/model1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: [ 0.60350877  0.56603774]\n",
      "\n",
      "Recall: [ 0.45382586  0.70496084]\n",
      "\n",
      "f1_score: [ 0.51807229  0.62790698]\n",
      "\n",
      "[[172 207]\n",
      " [113 270]]\n",
      ":: Classification Report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.6035    0.4538    0.5181       379\n",
      "          1     0.5660    0.7050    0.6279       383\n",
      "\n",
      "avg / total     0.5847    0.5801    0.5733       762\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temp = model.predict([x_tweet_val,x_hashtags_val])\n",
    "y_pred  = np.argmax(temp, 1)\n",
    "y_true = np.argmax(y_val, 1)\n",
    "precision = metrics.precision_score(y_true, y_pred, average=None)\n",
    "recall = metrics.recall_score(y_true, y_pred, average=None)\n",
    "f1_score = metrics.f1_score(y_true, y_pred, average=None)\n",
    "print(\"Precision: \" + str(precision) + \"\\n\")\n",
    "print(\"Recall: \" + str(recall) + \"\\n\")\n",
    "print(\"f1_score: \" + str(f1_score) + \"\\n\")\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "print(\":: Classification Report\")\n",
    "print(classification_report(y_true, y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_holographic_output(x_tweet, x_hashtags):\n",
    "    intermediate_layer_model = Model(inputs=model.input, outputs=model.get_layer('lambda_1').output)\n",
    "    intermediate_output = intermediate_layer_model.predict([x_tweet, x_hashtags])\n",
    "    return np.array(list(intermediate_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_xgb = np.concatenate((get_holographic_output(x_tweet_train, x_hashtags_train),\n",
    "#                               W_hash_tr,W_at_tr,W_exc_tr),axis=1)\n",
    "# X_val_xgb = np.concatenate((get_holographic_output(x_tweet_val, x_hashtags_val),\n",
    "#                             W_hash_dev,W_at_dev,W_exc_dev),axis=1)\n",
    "X_train_xgb = get_holographic_output(x_tweet_train,x_hashtags_train)\n",
    "X_val_xgb = get_holographic_output(x_tweet_val,x_hashtags_val)\n",
    "y_train_xgb = np.argmax(y_train, axis=1)\n",
    "y_val_xgb = np.argmax(y_val, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('holographic',indices,X_train_xgb,X_val_xgb,y_train_xgb,y_val_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: [ 0.57783641  0.58224543]\n",
      "\n",
      "Recall: [ 0.57783641  0.58224543]\n",
      "\n",
      "f1_score: [ 0.57783641  0.58224543]\n",
      "\n",
      "[[219 160]\n",
      " [160 223]]\n",
      ":: Classification Report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.5778    0.5778    0.5778       379\n",
      "          1     0.5822    0.5822    0.5822       383\n",
      "\n",
      "avg / total     0.5801    0.5801    0.5801       762\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "logreg = xgb.XGBClassifier(max_depth=3, learning_rate=0.1, n_estimators=100, silent=False, \n",
    "                           objective='binary:logistic', booster='gbtree', n_jobs=1, nthread=None, \n",
    "                           gamma=0, min_child_weight=1, max_delta_step=0, subsample=1, colsample_bytree=1, \n",
    "                           colsample_bylevel=1, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, base_score=0.5, \n",
    "                           random_state=0, seed=None, missing=None)\n",
    "logreg.fit(X_train_xgb, y_train_xgb)\n",
    "\n",
    "y_pred = logreg.predict(X_val_xgb)\n",
    "y_true = y_val_xgb\n",
    "precision = metrics.precision_score(y_true, y_pred, average=None)\n",
    "recall = metrics.recall_score(y_true, y_pred, average=None)\n",
    "f1_score = metrics.f1_score(y_true, y_pred, average=None)\n",
    "print(\"Precision: \" + str(precision) + \"\\n\")\n",
    "print(\"Recall: \" + str(recall) + \"\\n\")\n",
    "print(\"f1_score: \" + str(f1_score) + \"\\n\")\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "print(\":: Classification Report\")\n",
    "print(classification_report(y_true, y_pred, digits=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
