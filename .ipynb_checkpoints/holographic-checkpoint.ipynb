{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "import re\n",
    "import os\n",
    "import wordsegment as ws\n",
    "import preprocessor as p\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from keras.layers import Embedding, Dense, Input, Flatten, MaxPooling1D, Embedding, Merge, Dropout, LSTM, Bidirectional\n",
    "from keras.models import Model,Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras import initializers, optimizers\n",
    "from keras import backend as K\n",
    "\n",
    "os.environ['KERAS_BACKEND']='tensorflow'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./datasets/train/SemEval2018-T3-train-taskA.txt\", sep=\"\\t\")\n",
    "ws.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hashtags(tweet):\n",
    "    parsed_tweet = p.parse(tweet.lower())\n",
    "    parsed_hashtags = parsed_tweet.hashtags\n",
    "    all_hashtags = {}\n",
    "    \n",
    "    hashtags = []\n",
    "    if parsed_hashtags is not None:\n",
    "        for h in parsed_hashtags:\n",
    "            temp = h.match[1:].lower()\n",
    "            hashtag = \" \".join(ws.segment(temp))\n",
    "            if hashtag in all_hashtags:\n",
    "                all_hashtags[hashtag] += 1\n",
    "            else:\n",
    "                all_hashtags[hashtag] = 1\n",
    "            hashtags.append(hashtag)\n",
    "\n",
    "    hashtags_str = (\" \").join(hashtags)\n",
    "    return hashtags_str, len(hashtags), all_hashtags\n",
    "\n",
    "def get_text(tweet):\n",
    "    clean_tweet = p.clean(tweet)\n",
    "    clean_tweet = re.sub(r'[^\\w\\s]','',clean_tweet)\n",
    "    return clean_tweet.lower()\n",
    "\n",
    "\n",
    "def get_emotion(tweet):\n",
    "    emotion_keys = {}\n",
    "    result = re.findall(r\":\\w+_\\w+:\",tweet)\n",
    "    if result is not None:\n",
    "        emotions = []\n",
    "        for i in range(len(result)):\n",
    "            emotion = result[i][1:-1]\n",
    "            emotions.append(emotion)\n",
    "            if emotion in emotion_keys:\n",
    "                emotion_keys[emotion] += 1\n",
    "            else:\n",
    "                emotion_keys[emotion] = 1\n",
    "    return emotions, emotion_keys "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet index</th>\n",
       "      <th>Label</th>\n",
       "      <th>Tweet text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>length</th>\n",
       "      <th>hashtag_dict</th>\n",
       "      <th>tweet</th>\n",
       "      <th>emotion</th>\n",
       "      <th>emotion_dict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Sweet United Nations video. Just in time for Christmas. #imagine #NoReligion  http://t.co/fej2v3OUBR</td>\n",
       "      <td>imagine no religion</td>\n",
       "      <td>2</td>\n",
       "      <td>{'imagine': 1, 'no religion': 1}</td>\n",
       "      <td>sweet united nations video just in time for christmas</td>\n",
       "      <td>[]</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>@mrdahl87 We are rumored to have talked to Erv's agent... and the Angels asked about Ed Escobar... that's hardly nothing    ;)</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "      <td>we are rumored to have talked to ervs agent and the angels asked about ed escobar thats hardly nothing</td>\n",
       "      <td>[]</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Hey there! Nice to see you Minnesota/ND Winter Weather</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "      <td>hey there nice to see you minnesotand winter weather</td>\n",
       "      <td>[]</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3 episodes left I'm dying over here</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "      <td>3 episodes left im dying over here</td>\n",
       "      <td>[]</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>I can't breathe! was chosen as the most notable quote of the year in an annual list released by a Yale University librarian</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "      <td>i cant breathe was chosen as the most notable quote of the year in an annual list released by a yale university librarian</td>\n",
       "      <td>[]</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Tweet index  Label  \\\n",
       "0  1            1       \n",
       "1  2            1       \n",
       "2  3            1       \n",
       "3  4            0       \n",
       "4  5            1       \n",
       "\n",
       "                                                                                                                       Tweet text  \\\n",
       "0  Sweet United Nations video. Just in time for Christmas. #imagine #NoReligion  http://t.co/fej2v3OUBR                             \n",
       "1  @mrdahl87 We are rumored to have talked to Erv's agent... and the Angels asked about Ed Escobar... that's hardly nothing    ;)   \n",
       "2  Hey there! Nice to see you Minnesota/ND Winter Weather                                                                           \n",
       "3  3 episodes left I'm dying over here                                                                                              \n",
       "4  I can't breathe! was chosen as the most notable quote of the year in an annual list released by a Yale University librarian      \n",
       "\n",
       "              hashtags  length                      hashtag_dict  \\\n",
       "0  imagine no religion  2       {'imagine': 1, 'no religion': 1}   \n",
       "1                       0       {}                                 \n",
       "2                       0       {}                                 \n",
       "3                       0       {}                                 \n",
       "4                       0       {}                                 \n",
       "\n",
       "                                                                                                                       tweet  \\\n",
       "0  sweet united nations video just in time for christmas                                                                       \n",
       "1  we are rumored to have talked to ervs agent and the angels asked about ed escobar thats hardly nothing                      \n",
       "2  hey there nice to see you minnesotand winter weather                                                                        \n",
       "3  3 episodes left im dying over here                                                                                          \n",
       "4  i cant breathe was chosen as the most notable quote of the year in an annual list released by a yale university librarian   \n",
       "\n",
       "  emotion emotion_dict  \n",
       "0  []      {}           \n",
       "1  []      {}           \n",
       "2  []      {}           \n",
       "3  []      {}           \n",
       "4  []      {}           "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['hashtags'], data['length'], data['hashtag_dict'] = zip(*data['Tweet text'].map(get_hashtags)) \n",
    "data[\"tweet\"] = data['Tweet text'].map(get_text)\n",
    "data['emotion'], data['emotion_dict'] = zip(*data['tweet'].map(get_emotion))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = ''\n",
    "GLOVE_DIR = os.path.join(BASE_DIR, 'glove')\n",
    "TEXT_LENGTH = max(len(x.split(' ')) for x in data['tweet'].tolist())\n",
    "HASHTAG_LENGTH = max(len(x.split(' ')) for x in data['hashtags'].tolist())\n",
    "MAX_NUM_WORDS = 20000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2\n",
    "NUM_CLASSES = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors.\n",
      "Found 1193514 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# first, build index mapping words in the embeddings set\n",
    "# to their embedding vector\n",
    "import io\n",
    "\n",
    "print('Indexing word vectors.')\n",
    "\n",
    "embeddings_index = {}\n",
    "f = io.open(os.path.join(GLOVE_DIR, 'glove.twitter.27B.100d.txt'),encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8562 unique tokens.\n",
      "Found 2833 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "# vectorize the text samples into a 2D integer tensor\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "def vectorize_data(text, MAX_NUM_WORDS, MAX_SEQUENCE_LENGTH):\n",
    "    tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "    tokenizer.fit_on_texts(text)\n",
    "    sequences = tokenizer.texts_to_sequences(text)\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "    data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    return data, word_index\n",
    "\n",
    "labels = to_categorical(np.asarray(data['Label']))\n",
    "x_tweet, tweet_token_size = vectorize_data(data['tweet'],MAX_NUM_WORDS,TEXT_LENGTH)\n",
    "x_hashtags, ht_token_size = vectorize_data(data['hashtags'],MAX_NUM_WORDS,HASHTAG_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into a training set and a validation set\n",
    "indices = np.arange(x_tweet.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "x_tweet = x_tweet[indices]\n",
    "x_hashtags = x_hashtags[indices]\n",
    "labels = labels[indices]\n",
    "num_validation_samples = int(VALIDATION_SPLIT * x_tweet.shape[0])\n",
    "\n",
    "x_tweet_train = x_tweet[:-num_validation_samples]\n",
    "x_hashtags_train = x_hashtags[:-num_validation_samples]\n",
    "y_train = labels[:-num_validation_samples]\n",
    "x_tweet_val = x_tweet[-num_validation_samples:]\n",
    "x_hashtags_val = x_hashtags[-num_validation_samples:]\n",
    "y_val = labels[-num_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare embedding matrix\n",
    "def get_embedding_matrix(word_index):\n",
    "    num_words = min(MAX_NUM_WORDS, len(word_index))\n",
    "    embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= MAX_NUM_WORDS:\n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i-1] = embedding_vector\n",
    "    return embedding_matrix\n",
    "\n",
    "tweet_emb = get_embedding_matrix(tweet_token_size)\n",
    "hashtag_emb = get_embedding_matrix(ht_token_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blstm_maxpool(x, word_index, emb_matrix, num_filters, max_seq_len, learn_rate):\n",
    "    num_words = min(MAX_NUM_WORDS, len(word_index))\n",
    "    y = Embedding(num_words,EMBEDDING_DIM,weights=[emb_matrix],\n",
    "                                        input_length=max_seq_len,trainable=False)(x)\n",
    "    y = Bidirectional(LSTM(num_filters, return_sequences=True))(y)\n",
    "    y = MaxPooling1D(pool_size=max_seq_len, strides=None, padding='valid')(y)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_FILTERS = 100\n",
    "LEARNING_RATE = 0.005\n",
    "tweet = Input(shape=(TEXT_LENGTH,), dtype='int32')\n",
    "hashtag = Input(shape=(HASHTAG_LENGTH,), dtype='int32')\n",
    "\n",
    "tweet_lstm_vec = blstm_maxpool(tweet,tweet_token_size,tweet_emb, NUM_FILTERS, TEXT_LENGTH, LEARNING_RATE)\n",
    "ht_lstm_vec = blstm_maxpool(hashtag,ht_token_size,hashtag_emb, NUM_FILTERS, HASHTAG_LENGTH, LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class HolographicMerge(Layer):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(HolographicMerge, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Create a trainable weight variable for this layer.\n",
    "        self.W = self.add_weight(name='kernel', \n",
    "                                      shape=(input_shape[-1],),\n",
    "                                      initializer='random_normal',\n",
    "                                      trainable=True)\n",
    "        super(HolographicMerge, self).build(input_shape)  # Be sure to call this somewhere!\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        eij = K.tanh(K.dot(x, self.W))\n",
    "        \n",
    "        ai = K.exp(eij)\n",
    "        weights = ai/K.sum(ai, axis=1).dimshuffle(0,'x')\n",
    "        \n",
    "        weighted_input = x*weights.dimshuffle(0,1,'x')\n",
    "        return weighted_input.sum(axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])\n",
    "\n",
    "\n",
    "def holographic_merge(a,b):\n",
    "    a_fft = tf.fft(tf.complex(a, 0.0))\n",
    "    b_fft = tf.fft(tf.complex(b, 0.0))\n",
    "    ifft = tf.ifft(tf.conj(a_fft) * b_fft)\n",
    "    return tf.cast(tf.real(ifft), 'float32') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_circ = holographic_merge(tweet_lstm_vec,ht_lstm_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropout and dense layer\n",
    "\n",
    "h_circ = Dropout(0.3)(h_circ)\n",
    "preds = Dense(NUM_CLASSES, activation='softmax')(h_circ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute '_keras_history'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-69-415ec9749ed0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtweet\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhashtag\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpreds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\rdesh\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 87\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\rdesh\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\topology.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, inputs, outputs, name)\u001b[0m\n\u001b[0;32m   1714\u001b[0m         \u001b[0mnodes_in_progress\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1715\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1716\u001b[1;33m             \u001b[0mbuild_map_of_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinished_nodes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnodes_in_progress\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1717\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1718\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mnode\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnodes_in_decreasing_depth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\rdesh\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\topology.py\u001b[0m in \u001b[0;36mbuild_map_of_graph\u001b[1;34m(tensor, finished_nodes, nodes_in_progress, layer, node_index, tensor_index)\u001b[0m\n\u001b[0;32m   1704\u001b[0m                 \u001b[0mtensor_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor_indices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1705\u001b[0m                 build_map_of_graph(x, finished_nodes, nodes_in_progress,\n\u001b[1;32m-> 1706\u001b[1;33m                                    layer, node_index, tensor_index)\n\u001b[0m\u001b[0;32m   1707\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1708\u001b[0m             \u001b[0mfinished_nodes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\rdesh\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\topology.py\u001b[0m in \u001b[0;36mbuild_map_of_graph\u001b[1;34m(tensor, finished_nodes, nodes_in_progress, layer, node_index, tensor_index)\u001b[0m\n\u001b[0;32m   1704\u001b[0m                 \u001b[0mtensor_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor_indices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1705\u001b[0m                 build_map_of_graph(x, finished_nodes, nodes_in_progress,\n\u001b[1;32m-> 1706\u001b[1;33m                                    layer, node_index, tensor_index)\n\u001b[0m\u001b[0;32m   1707\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1708\u001b[0m             \u001b[0mfinished_nodes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\rdesh\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\topology.py\u001b[0m in \u001b[0;36mbuild_map_of_graph\u001b[1;34m(tensor, finished_nodes, nodes_in_progress, layer, node_index, tensor_index)\u001b[0m\n\u001b[0;32m   1675\u001b[0m             \"\"\"\n\u001b[0;32m   1676\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mnode_index\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mtensor_index\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1677\u001b[1;33m                 \u001b[0mlayer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnode_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_keras_history\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1678\u001b[0m             \u001b[0mnode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minbound_nodes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnode_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Tensor' object has no attribute '_keras_history'"
     ]
    }
   ],
   "source": [
    "model = Model([tweet,hashtag],preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
