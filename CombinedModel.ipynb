{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, confusion_matrix \n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.utils import shuffle\n",
    "from string import punctuation\n",
    "import re\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn import metrics\n",
    "\n",
    "import os\n",
    "os.environ['KERAS_BACKEND']='theano'\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Merge, Dropout, LSTM, GRU, Bidirectional, Activation\n",
    "from keras.models import Model,Sequential\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras import initializers, optimizers\n",
    "from keras.models import load_model\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data(filename):\n",
    "    data = {}\n",
    "    with open(filename) as f:\n",
    "        d = np.load(f)\n",
    "        data[\"indices\"] = d['arr_0']\n",
    "        data[\"X_train\"] = d['arr_1']\n",
    "        data[\"X_test\"] = d['arr_2']\n",
    "        data[\"y_train\"] = d['arr_3']\n",
    "        data[\"y_test\"] = d['arr_4']\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttLayer(Layer):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Create a trainable weight variable for this layer.\n",
    "        self.W = self.add_weight(name='kernel', \n",
    "                                      shape=(input_shape[-1],),\n",
    "                                      initializer='random_normal',\n",
    "                                      trainable=True)\n",
    "        super(AttLayer, self).build(input_shape)  # Be sure to call this somewhere!\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        eij = K.tanh(K.dot(x, self.W))\n",
    "        \n",
    "        ai = K.exp(eij)\n",
    "        weights = ai/K.sum(ai, axis=1).dimshuffle(0,'x')\n",
    "        \n",
    "        weighted_input = x*weights.dimshuffle(0,1,'x')\n",
    "        return weighted_input.sum(axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])\n",
    "\n",
    "model = load_model(\"trainedmodels/sarc_transfer_all.h5\", custom_objects={'AttLayer':AttLayer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_1 = get_data('./trainedmodels/char_ngrams_tfidf.npz')\n",
    "data_2 = get_data('./trainedmodels/holographic.npz')\n",
    "data_3 = get_data('./trainedmodels/sarc_transfer_all.npz')\n",
    "data_4 = get_data('./trainedmodels/deepmoji.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_results_XGB(data_1, data_2):\n",
    "    \n",
    "    trainX = np.hstack((data_1[\"X_train\"], data_2[\"X_train\"]))\n",
    "    trainY = data_1[\"y_train\"]\n",
    "    testX = np.hstack((data_1[\"X_test\"], data_2[\"X_test\"]))\n",
    "    testY = data_1[\"y_test\"]\n",
    "    \n",
    "    logreg = xgb.XGBClassifier()\n",
    "    logreg.fit(trainX, trainY)\n",
    "\n",
    "    y_pred = logreg.predict(testX)\n",
    "    y_true = testY\n",
    "    precision = metrics.precision_score(y_true, y_pred, average=None)\n",
    "    recall = metrics.recall_score(y_true, y_pred, average=None)\n",
    "    f1_score = metrics.f1_score(y_true, y_pred, average=None)\n",
    "    print(\"Precision: \" + str(precision) + \"\\n\")\n",
    "    print(\"Recall: \" + str(recall) + \"\\n\")\n",
    "    print(\"f1_score: \" + str(f1_score) + \"\\n\")\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "    print(\":: Classification Report\")\n",
    "    print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_results_LR(data):\n",
    "    \n",
    "    X_train = data[\"X_train\"]\n",
    "    Y_train = data[\"y_train\"]\n",
    "    X_test = data[\"X_test\"]\n",
    "    Y_test = data[\"y_test\"]\n",
    "    \n",
    "    logreg = LogisticRegression(class_weight=\"balanced\")\n",
    "    logreg.fit(X_train, Y_train)\n",
    "\n",
    "    y_pred = logreg.predict(X_test)\n",
    "    y_true = Y_test\n",
    "    precision = metrics.precision_score(y_true, y_pred, average=None)\n",
    "    recall = metrics.recall_score(y_true, y_pred, average=None)\n",
    "    f1_score = metrics.f1_score(y_true, y_pred, average=None)\n",
    "    accuracy = metrics.accuracy_score(y_true, y_pred)\n",
    "    print(\"Precision: \" + str(precision) + \"\\n\")\n",
    "    print(\"Recall: \" + str(recall) + \"\\n\")\n",
    "    print(\"f1_score: \" + str(f1_score) + \"\\n\")\n",
    "    print(\"Accuracy: \" + str(accuracy) + \"\\n\")\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "    print(\":: Classification Report\")\n",
    "    print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_results_XGB(data_2, data_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "While loading data in the dataframe, some lines are incorrectly read, i.e., their tweet length is >140 since multiple tweets are read as single record. I have removed these records as:\n",
    "\"\"\"\n",
    "data = pd.read_csv(\"./datasets/train/SemEval2018-T3-train-taskA_emoji.txt\", sep=\"\\t\")\n",
    "data = data[data['Tweet text'].map(len)<=140]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
