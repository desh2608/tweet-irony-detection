{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "import re\n",
    "import os\n",
    "import wordsegment as ws\n",
    "import preprocessor as p\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from keras.layers import Embedding, Dense, Input, MaxPooling2D, Dropout, LSTM, Bidirectional, Reshape\n",
    "from keras.models import Model,Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras import initializers, optimizers\n",
    "from keras import backend as K\n",
    "\n",
    "os.environ['KERAS_BACKEND']='tensorflow'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./datasets/test/SemEval2018-T3_input_test_taskB_emoji.txt\", sep=\"\\t\")\n",
    "ws.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hashtags(tweet):\n",
    "    parsed_tweet = p.parse(tweet.lower())\n",
    "    parsed_hashtags = parsed_tweet.hashtags\n",
    "    all_hashtags = {}\n",
    "    \n",
    "    hashtags = []\n",
    "    if parsed_hashtags is not None:\n",
    "        for h in parsed_hashtags:\n",
    "            temp = h.match[1:].lower()\n",
    "            hashtag = \" \".join(ws.segment(temp))\n",
    "            if hashtag in all_hashtags:\n",
    "                all_hashtags[hashtag] += 1\n",
    "            else:\n",
    "                all_hashtags[hashtag] = 1\n",
    "            hashtags.append(hashtag)\n",
    "\n",
    "    hashtags_str = (\" \").join(hashtags)\n",
    "    return hashtags_str, len(hashtags), all_hashtags\n",
    "\n",
    "def get_text(tweet):\n",
    "    clean_tweet = p.clean(tweet)\n",
    "    clean_tweet = re.sub(r'[^\\w\\s]','',clean_tweet)\n",
    "    return clean_tweet.lower()\n",
    "\n",
    "\n",
    "def get_emotion(tweet):\n",
    "    emotion_keys = {}\n",
    "    result = re.findall(r\":\\w+_\\w+:\",tweet)\n",
    "    if result is not None:\n",
    "        emotions = []\n",
    "        for i in range(len(result)):\n",
    "            emotion = result[i][1:-1]\n",
    "            emotions.append(emotion)\n",
    "            if emotion in emotion_keys:\n",
    "                emotion_keys[emotion] += 1\n",
    "            else:\n",
    "                emotion_keys[emotion] = 1\n",
    "    return emotions, emotion_keys "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet index</th>\n",
       "      <th>tweet text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>length</th>\n",
       "      <th>hashtag_dict</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>@Callisto1947 Can U Help?||More conservatives needed on #TSU + get paid 4 posting stuff like this!||YOU $ can go to http://t.co/JUmMWi0AyT</td>\n",
       "      <td>tsu</td>\n",
       "      <td>1</td>\n",
       "      <td>{'tsu': 1}</td>\n",
       "      <td>can u helpmore conservatives needed on  get paid 4 posting stuff like thisyou  can go to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Just walked in to #Starbucks and asked for a \"tall blonde\" Hahahaha</td>\n",
       "      <td>starbucks</td>\n",
       "      <td>1</td>\n",
       "      <td>{'starbucks': 1}</td>\n",
       "      <td>just walked in to and asked for a tall blonde hahahaha</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>GONNA WIN http://t.co/Mc9ebqjAqj</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "      <td>gonna win</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>@mickymantell He is exactly that sort of person. Weirdo!</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "      <td>he is exactly that sort of person weirdo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>So much at work mate 10/10 #boring 100% #dead mate full on #shit absolutely #sleeping mate can't handle the</td>\n",
       "      <td>boring dead shit sleeping</td>\n",
       "      <td>4</td>\n",
       "      <td>{'sleeping': 1, 'dead': 1, 'boring': 1, 'shit': 1}</td>\n",
       "      <td>so much at work mate 1010 100 mate full on absolutely mate cant handle the</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet index  \\\n",
       "0  1             \n",
       "1  2             \n",
       "2  3             \n",
       "3  4             \n",
       "4  5             \n",
       "\n",
       "                                                                                                                                   tweet text  \\\n",
       "0  @Callisto1947 Can U Help?||More conservatives needed on #TSU + get paid 4 posting stuff like this!||YOU $ can go to http://t.co/JUmMWi0AyT   \n",
       "1  Just walked in to #Starbucks and asked for a \"tall blonde\" Hahahaha                                                                          \n",
       "2  GONNA WIN http://t.co/Mc9ebqjAqj                                                                                                             \n",
       "3  @mickymantell He is exactly that sort of person. Weirdo!                                                                                     \n",
       "4  So much at work mate 10/10 #boring 100% #dead mate full on #shit absolutely #sleeping mate can't handle the                                  \n",
       "\n",
       "                    hashtags  length  \\\n",
       "0  tsu                        1        \n",
       "1  starbucks                  1        \n",
       "2                             0        \n",
       "3                             0        \n",
       "4  boring dead shit sleeping  4        \n",
       "\n",
       "                                         hashtag_dict  \\\n",
       "0  {'tsu': 1}                                           \n",
       "1  {'starbucks': 1}                                     \n",
       "2  {}                                                   \n",
       "3  {}                                                   \n",
       "4  {'sleeping': 1, 'dead': 1, 'boring': 1, 'shit': 1}   \n",
       "\n",
       "                                                                                      tweet  \n",
       "0  can u helpmore conservatives needed on  get paid 4 posting stuff like thisyou  can go to  \n",
       "1  just walked in to and asked for a tall blonde hahahaha                                    \n",
       "2  gonna win                                                                                 \n",
       "3  he is exactly that sort of person weirdo                                                  \n",
       "4  so much at work mate 1010 100 mate full on absolutely mate cant handle the                "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['hashtags'], data['length'], data['hashtag_dict'] = zip(*data['tweet text'].map(get_hashtags)) \n",
    "data[\"tweet\"] = data['tweet text'].map(get_text)\n",
    "# data['emotion'], data['emotion_dict'] = zip(*data['tweet'].map(get_emotion))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = ''\n",
    "GLOVE_DIR = os.path.join(BASE_DIR, 'glove')\n",
    "# TEXT_LENGTH = max(len(x.split(' ')) for x in data['tweet'].tolist())\n",
    "# HASHTAG_LENGTH = max(len(x.split(' ')) for x in data['hashtags'].tolist())\n",
    "MAX_NUM_WORDS = 20000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2\n",
    "NUM_CLASSES = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors.\n",
      "Found 1193514 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# first, build index mapping words in the embeddings set\n",
    "# to their embedding vector\n",
    "import io\n",
    "\n",
    "print('Indexing word vectors.')\n",
    "\n",
    "embeddings_index = {}\n",
    "f = io.open(os.path.join(GLOVE_DIR, 'glove.twitter.27B.100d.txt'),encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2866 unique tokens.\n",
      "Found 1016 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "# vectorize the text samples into a 2D integer tensor\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "def vectorize_data(text, MAX_NUM_WORDS, MAX_SEQUENCE_LENGTH):\n",
    "    tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "    tokenizer.fit_on_texts(text)\n",
    "    sequences = tokenizer.texts_to_sequences(text)\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    print('Found %s unique tokens.' % len(word_index))\n",
    "    \n",
    "    for word,idx in word_index.items():\n",
    "        word_index[word] = idx - 1\n",
    "\n",
    "    data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    return data, word_index\n",
    "\n",
    "# labels = to_categorical(np.asarray(data['Label']))\n",
    "x_tweet, tweet_token_index = vectorize_data(data['tweet'],MAX_NUM_WORDS,TEXT_LENGTH)\n",
    "x_hashtags, ht_token_index = vectorize_data(data['hashtags'],MAX_NUM_WORDS,HASHTAG_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into a training set and a validation set\n",
    "indices = np.arange(x_tweet.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "x_tweet = x_tweet[indices]\n",
    "x_hashtags = x_hashtags[indices]\n",
    "labels = labels[indices]\n",
    "# num_validation_samples = int(VALIDATION_SPLIT * x_tweet.shape[0])\n",
    "\n",
    "# x_tweet_train = x_tweet[:-num_validation_samples]\n",
    "# x_hashtags_train = x_hashtags[:-num_validation_samples]\n",
    "# y_train = labels[:-num_validation_samples]\n",
    "# x_tweet_val = x_tweet[-num_validation_samples:]\n",
    "# x_hashtags_val = x_hashtags[-num_validation_samples:]\n",
    "# y_val = labels[-num_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare embedding matrix\n",
    "def get_embedding_matrix(word_index):\n",
    "    num_words = min(MAX_NUM_WORDS, len(word_index)+1)\n",
    "    embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= MAX_NUM_WORDS:\n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i-1] = embedding_vector\n",
    "    return embedding_matrix\n",
    "\n",
    "tweet_emb = get_embedding_matrix(tweet_token_index)\n",
    "hashtag_emb = get_embedding_matrix(ht_token_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blstm_maxpool(x, word_index, emb_matrix, num_filters, max_seq_len, learn_rate):\n",
    "    num_words = min(MAX_NUM_WORDS, len(word_index)+1)\n",
    "    y = Embedding(num_words,EMBEDDING_DIM,weights=[emb_matrix],\n",
    "                                        input_length=max_seq_len,trainable=False)(x)\n",
    "    y = Bidirectional(LSTM(num_filters, return_sequences=True))(y)\n",
    "    y = Reshape((max_seq_len,2*num_filters,1))(y)\n",
    "    y = MaxPooling2D(pool_size=(max_seq_len,1), strides=None, padding='valid')(y)\n",
    "    y = Reshape((2*num_filters,))(y)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_FILTERS = 100\n",
    "LEARNING_RATE = 0.005\n",
    "tweet = Input(batch_shape=(None,TEXT_LENGTH), dtype='int32')\n",
    "hashtag = Input(batch_shape=(None,HASHTAG_LENGTH), dtype='int32')\n",
    "\n",
    "tweet_lstm_vec = blstm_maxpool(tweet,tweet_token_index,tweet_emb, NUM_FILTERS, TEXT_LENGTH, LEARNING_RATE)\n",
    "ht_lstm_vec = blstm_maxpool(hashtag,ht_token_index,hashtag_emb, NUM_FILTERS, HASHTAG_LENGTH, LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def holographic_merge(inp):\n",
    "    [a, b] = inp\n",
    "    a_fft = tf.fft(tf.complex(a, 0.0))\n",
    "    b_fft = tf.fft(tf.complex(b, 0.0))\n",
    "    ifft = tf.ifft(tf.conj(a_fft) * b_fft)\n",
    "    return tf.cast(tf.real(ifft), 'float32') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Lambda\n",
    "\n",
    "h_circ = Lambda(holographic_merge)([tweet_lstm_vec,ht_lstm_vec])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropout and dense layer\n",
    "\n",
    "h_circ = Dropout(0.3)(h_circ)\n",
    "preds = Dense(NUM_CLASSES, activation='softmax')(h_circ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model([tweet,hashtag],preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 32)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 23)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 32, 100)      822600      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 23, 100)      282500      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 32, 200)      160800      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 23, 200)      160800      embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 32, 200, 1)   0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)             (None, 23, 200, 1)   0           bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 1, 200, 1)    0           reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 1, 200, 1)    0           reshape_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 200)          0           max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "reshape_4 (Reshape)             (None, 200)          0           max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 200)          0           reshape_2[0][0]                  \n",
      "                                                                 reshape_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 200)          0           lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 4)            804         dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,427,504\n",
      "Trainable params: 322,404\n",
      "Non-trainable params: 1,105,100\n",
      "__________________________________________________________________________________________________\n",
      "Training\n",
      "Epoch 1/10\n",
      "3813/3813 [==============================] - 43s 11ms/step - loss: 1.1458 - acc: 0.4823\n",
      "Epoch 2/10\n",
      "3813/3813 [==============================] - 26s 7ms/step - loss: 1.0236 - acc: 0.5389\n",
      "Epoch 3/10\n",
      "3813/3813 [==============================] - 27s 7ms/step - loss: 0.9767 - acc: 0.5754\n",
      "Epoch 4/10\n",
      "3813/3813 [==============================] - 26s 7ms/step - loss: 0.8960 - acc: 0.6234\n",
      "Epoch 5/10\n",
      "3813/3813 [==============================] - 25s 7ms/step - loss: 0.8421 - acc: 0.6431\n",
      "Epoch 6/10\n",
      "3813/3813 [==============================] - 27s 7ms/step - loss: 0.7224 - acc: 0.7202\n",
      "Epoch 7/10\n",
      "3813/3813 [==============================] - 27s 7ms/step - loss: 0.6154 - acc: 0.7558\n",
      "Epoch 8/10\n",
      "3813/3813 [==============================] - 28s 7ms/step - loss: 0.4938 - acc: 0.8088\n",
      "Epoch 9/10\n",
      "3813/3813 [==============================] - 27s 7ms/step - loss: 0.3926 - acc: 0.8563\n",
      "Epoch 10/10\n",
      "3813/3813 [==============================] - 28s 7ms/step - loss: 0.2764 - acc: 0.9030\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f502db9f98>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 20\n",
    "EPOCHS = 10\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "print('Training')\n",
    "model.fit([x_tweet, x_hashtags], labels,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=EPOCHS,\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: [ 0.6614786   0.55795678]\n",
      "\n",
      "Recall: [ 0.43037975  0.76549865]\n",
      "\n",
      "f1_score: [ 0.52147239  0.64545455]\n",
      "\n",
      "[[170 225]\n",
      " [ 87 284]]\n",
      ":: Classification Report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.6615    0.4304    0.5215       395\n",
      "          1     0.5580    0.7655    0.6455       371\n",
      "\n",
      "avg / total     0.6113    0.5927    0.5815       766\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temp = model.predict([x_tweet_val,x_hashtags_val])\n",
    "y_pred  = np.argmax(temp, 1)\n",
    "y_true = np.argmax(y_val, 1)\n",
    "precision = metrics.precision_score(y_true, y_pred, average=None)\n",
    "recall = metrics.recall_score(y_true, y_pred, average=None)\n",
    "f1_score = metrics.f1_score(y_true, y_pred, average=None)\n",
    "print(\"Precision: \" + str(precision) + \"\\n\")\n",
    "print(\"Recall: \" + str(recall) + \"\\n\")\n",
    "print(\"f1_score: \" + str(f1_score) + \"\\n\")\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "print(\":: Classification Report\")\n",
    "print(classification_report(y_true, y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting holographic embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_holographic_output(x_tweet, x_hashtags):\n",
    "    intermediate_layer_model = Model(inputs=model.input, outputs=model.get_layer('lambda_1').output)\n",
    "    intermediate_output = intermediate_layer_model.predict([x_tweet, x_hashtags])\n",
    "    return np.array(list(intermediate_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices, x_tweet, x_hashtags, labels = (list(t) for t in zip(*sorted(zip(indices, x_tweet, x_hashtags, labels))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = get_holographic_output(np.array(x_tweet),np.array(x_hashtags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('taskB_test_holographic',X_test)\n",
    "np.save('taskB_labels',labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
