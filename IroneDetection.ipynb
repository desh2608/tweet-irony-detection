{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "/usr/local/lib/python2.7/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import preprocessor as p\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "import preprocessor as p\n",
    "from collections import Counter\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix \n",
    "from tensorflow.contrib import learn\n",
    "from tflearn.data_utils import to_categorical, pad_sequences\n",
    "import os\n",
    "os.environ['KERAS_BACKEND']='theano'\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Merge, Dropout, LSTM, GRU, Bidirectional\n",
    "from keras.models import Model,Sequential\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras import initializers, optimizers\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Irony Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_hashtags = {}\n",
    "def get_hashtags(tweet):\n",
    "    parsed_tweet = p.parse(tweet.decode('ascii', 'ignore').encode('ascii').lower())\n",
    "    parsed_hashtags = parsed_tweet.hashtags\n",
    "    \n",
    "    hashtags = []\n",
    "    if parsed_hashtags is not None:\n",
    "        for hashtag in parsed_hashtags:\n",
    "            temp = hashtag.match[1:].lower()\n",
    "            if temp in all_hashtags:\n",
    "                all_hashtags[temp] += 1\n",
    "            else:\n",
    "                all_hashtags[temp] = 1\n",
    "            hashtags.append(temp)\n",
    "\n",
    "    hashtags_str = (\" \").join(hashtags)\n",
    "    return hashtags_str, len(hashtags)\n",
    "\n",
    "def get_clean_tweet(tweet):\n",
    "    p.set_options(p.OPT.URL)\n",
    "    clean_tweet = p.clean(tweet)\n",
    "    return clean_tweet.lower().replace(\"#\",\" \")\n",
    "\n",
    "\n",
    "emotion_keys = {}\n",
    "def get_emotion(tweet):\n",
    "    result = re.findall(r\":\\w+_\\w+:\",tweet)\n",
    "    if result is not None:\n",
    "        emotions = []\n",
    "        for i in range(len(result)):\n",
    "            emotion = result[i][1:-1]\n",
    "            emotions.append(emotion)\n",
    "            if emotion in emotion_keys:\n",
    "                emotion_keys[emotion] += 1\n",
    "            else:\n",
    "                emotion_keys[emotion] = 1\n",
    "    return (\" \").join(emotions)\n",
    "\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "def get_sentiment_score(tweet):\n",
    "    ss = sid.polarity_scores(tweet)\n",
    "    return ss[\"pos\"], ss[\"neg\"], ss[\"neu\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./datasets/train/SemEval2018-T3-train-taskA.txt\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet index</th>\n",
       "      <th>Label</th>\n",
       "      <th>Tweet text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>length</th>\n",
       "      <th>tweet</th>\n",
       "      <th>emotion</th>\n",
       "      <th>pos</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Sweet United Nations video. Just in time for Christmas. #imagine #NoReligion  http://t.co/fej2v3OUBR</td>\n",
       "      <td>imagine noreligion</td>\n",
       "      <td>2</td>\n",
       "      <td>sweet united nations video. just in time for christmas.  imagine  noreligion</td>\n",
       "      <td></td>\n",
       "      <td>0.392</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>@mrdahl87 We are rumored to have talked to Erv's agent... and the Angels asked about Ed Escobar... that's hardly nothing    ;)</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>@mrdahl87 we are rumored to have talked to erv's agent... and the angels asked about ed escobar... that's hardly nothing ;)</td>\n",
       "      <td></td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Hey there! Nice to see you Minnesota/ND Winter Weather</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>hey there! nice to see you minnesota/nd winter weather</td>\n",
       "      <td></td>\n",
       "      <td>0.279</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3 episodes left I'm dying over here</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>3 episodes left i'm dying over here</td>\n",
       "      <td></td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>I can't breathe! was chosen as the most notable quote of the year in an annual list released by a Yale University librarian</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>i can't breathe! was chosen as the most notable quote of the year in an annual list released by a yale university librarian</td>\n",
       "      <td></td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Tweet index  Label  \\\n",
       "0  1            1       \n",
       "1  2            1       \n",
       "2  3            1       \n",
       "3  4            0       \n",
       "4  5            1       \n",
       "\n",
       "                                                                                                                       Tweet text  \\\n",
       "0  Sweet United Nations video. Just in time for Christmas. #imagine #NoReligion  http://t.co/fej2v3OUBR                             \n",
       "1  @mrdahl87 We are rumored to have talked to Erv's agent... and the Angels asked about Ed Escobar... that's hardly nothing    ;)   \n",
       "2  Hey there! Nice to see you Minnesota/ND Winter Weather                                                                           \n",
       "3  3 episodes left I'm dying over here                                                                                              \n",
       "4  I can't breathe! was chosen as the most notable quote of the year in an annual list released by a Yale University librarian      \n",
       "\n",
       "             hashtags  length  \\\n",
       "0  imagine noreligion  2        \n",
       "1                      0        \n",
       "2                      0        \n",
       "3                      0        \n",
       "4                      0        \n",
       "\n",
       "                                                                                                                         tweet  \\\n",
       "0  sweet united nations video. just in time for christmas.  imagine  noreligion                                                  \n",
       "1  @mrdahl87 we are rumored to have talked to erv's agent... and the angels asked about ed escobar... that's hardly nothing ;)   \n",
       "2  hey there! nice to see you minnesota/nd winter weather                                                                        \n",
       "3  3 episodes left i'm dying over here                                                                                           \n",
       "4  i can't breathe! was chosen as the most notable quote of the year in an annual list released by a yale university librarian   \n",
       "\n",
       "  emotion    pos    neg    neu  \n",
       "0          0.392  0.000  0.608  \n",
       "1          0.000  0.065  0.935  \n",
       "2          0.279  0.000  0.721  \n",
       "3          0.000  0.000  1.000  \n",
       "4          0.000  0.000  1.000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['hashtags'], data['length'] = zip(*data['Tweet text'].map(get_hashtags)) \n",
    "data[\"tweet\"] = data['Tweet text'].map(get_clean_tweet)\n",
    "data['emotion'] = data['tweet'].map(get_emotion)\n",
    "data['pos'], data['neg'], data['neu'] = zip(*data['tweet'].map(get_sentiment_score)) \n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_text = np.array(data[\"tweet\"].tolist())\n",
    "label =  np.array(data[\"Label\"].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using word and character unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "print(\"Using char n-grams based features\")\n",
    "bow_transformer = CountVectorizer(max_features = 10000, ngram_range = (1,4)).fit(x_text)\n",
    "comments_bow = bow_transformer.transform(x_text)\n",
    "# tfidf_transformer = TfidfTransformer(norm = 'l2').fit(comments_bow)\n",
    "# comments_tfidf = tfidf_transformer.transform(comments_bow)\n",
    "features = comments_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(features, label, random_state=42, test_size=0.10)\n",
    "\n",
    "logreg = LogisticRegression(class_weight=\"balanced\")\n",
    "logreg.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = logreg.predict(X_test)\n",
    "y_true = Y_test\n",
    "precision = metrics.precision_score(y_true, y_pred, average=None)\n",
    "recall = metrics.recall_score(y_true, y_pred, average=None)\n",
    "f1_score = metrics.f1_score(y_true, y_pred, average=None)\n",
    "accuracy = metrics.accuracy_score(y_true, y_pred)\n",
    "print(\"Precision: \" + str(precision) + \"\\n\")\n",
    "print(\"Recall: \" + str(recall) + \"\\n\")\n",
    "print(\"f1_score: \" + str(f1_score) + \"\\n\")\n",
    "print(\"Accuracy: \" + str(accuracy) + \"\\n\")\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "print(\":: Classification Report\")\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load SARC data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "f = open(\"./datasets/SARC/data_without_ancestor.pkl\", \"r\")\n",
    "x_text = pickle.load(f)\n",
    "label = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Deep Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_embedding_weights(filename, sep):\n",
    "    embed_dict = {}\n",
    "    file = open(filename,'r')\n",
    "    for line in file.readlines():\n",
    "        row = line.strip().split(sep)\n",
    "        embed_dict[row[0]] = row[1:]\n",
    "    print('Loaded from file: ' + str(filename))\n",
    "    file.close()\n",
    "    return embed_dict\n",
    "\n",
    "def map_embedding_weights(embed, vocab, embed_size):\n",
    "    vocab_size = len(vocab)\n",
    "    embeddingWeights = np.zeros((vocab_size , embed_size))\n",
    "    n = 0\n",
    "    words_missed = []\n",
    "    for k, v in vocab.iteritems():\n",
    "        try:\n",
    "            embeddingWeights[v] = embed[k]\n",
    "        except:\n",
    "            n += 1\n",
    "            words_missed.append(k)\n",
    "            pass\n",
    "    print(\"%d embedding missed\"%n, \" of \" , vocab_size)\n",
    "    return embeddingWeights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AttLayer(Layer):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Create a trainable weight variable for this layer.\n",
    "        self.W = self.add_weight(name='kernel', \n",
    "                                      shape=(input_shape[-1],),\n",
    "                                      initializer='random_normal',\n",
    "                                      trainable=True)\n",
    "        super(AttLayer, self).build(input_shape)  # Be sure to call this somewhere!\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        eij = K.tanh(K.dot(x, self.W))\n",
    "        \n",
    "        ai = K.exp(eij)\n",
    "        weights = ai/K.sum(ai, axis=1).dimshuffle(0,'x')\n",
    "        \n",
    "        weighted_input = x*weights.dimshuffle(0,1,'x')\n",
    "        return weighted_input.sum(axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])\n",
    "\n",
    "def blstm_atten(inp_dim, vocab_size, embed_size, num_classes, learn_rate):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, embed_size, input_length=inp_dim))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Bidirectional(LSTM(embed_size, return_sequences=True)))\n",
    "    model.add(AttLayer())\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "def cnn_lstm(inp_dim, vocab_size, embed_size, num_classes, learn_rate):\n",
    "    # Convolution\n",
    "    kernel_size = 5\n",
    "    filters = 64\n",
    "    pool_size = 4\n",
    "\n",
    "    # LSTM\n",
    "    lstm_output_size = 70\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, embed_size, input_length=inp_dim))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv1D(filters,\n",
    "                     kernel_size,\n",
    "                     padding='valid',\n",
    "                     activation='relu',\n",
    "                     strides=1))\n",
    "    model.add(MaxPooling1D(pool_size=pool_size))\n",
    "    model.add(LSTM(lstm_output_size))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def blstm(inp_dim,vocab_size, embed_size, num_classes, learn_rate):   \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, embed_size, input_length=inp_dim, trainable=True))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Bidirectional(LSTM(embed_size)))\n",
    "    model.add(Dropout(0.50))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_document_length = 50\n",
    "num_classes = 2\n",
    "embed_size = 50\n",
    "n_epoch = 10\n",
    "batch_size = 16\n",
    "learn_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(x_text, label, random_state=42, test_size=0.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VALIDATION_SPLIT = 0.2\n",
    "indices = np.load(\"split.pkl\")\n",
    "x_text = x_text[indices]\n",
    "label = label[indices]\n",
    "num_validation_samples = int(VALIDATION_SPLIT * x_text.shape[0])\n",
    "\n",
    "X_train = x_text[:-num_validation_samples]\n",
    "Y_train = label[:-num_validation_samples]\n",
    "X_test = x_text[-num_validation_samples:]\n",
    "Y_test = label[-num_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 17911\n"
     ]
    }
   ],
   "source": [
    "trained_vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length, 5)\n",
    "# vocab_processor = vocab_processor.fit(x_text)\n",
    "trained_vocab_processor = trained_vocab_processor.restore(\"./Models/SARC_vocab_glove.pkl\")\n",
    "\n",
    "vocab_size = len(trained_vocab_processor.vocabulary_._mapping)\n",
    "print(\"Vocabulary Size: {:d}\".format(vocab_size))\n",
    "vocab = trained_vocab_processor.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length, 5, vocabulary=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = np.array(list(vocab_processor.transform(X_train)))\n",
    "testX = np.array(list(vocab_processor.transform(X_test)))\n",
    "\n",
    "trainY = np.asarray(Y_train)\n",
    "testY = np.asarray(Y_test)\n",
    "\n",
    "trainX = pad_sequences(trainX, maxlen=max_document_length, value=0.)\n",
    "testX = pad_sequences(testX, maxlen=max_document_length, value=0.)\n",
    "\n",
    "trainY = to_categorical(trainY, nb_classes=num_classes)\n",
    "testY = to_categorical(testY, nb_classes=num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"word_vectors/glove.twitter.27B.50d.txt\"\n",
    "sep = \" \"\n",
    "\n",
    "model = blstm_atten(trainX.shape[1], vocab_size, embed_size, num_classes, learn_rate)\n",
    "model.layers[0].set_weights([map_embedding_weights(get_embedding_weights(filename, sep), vocab, embed_size)])\n",
    "model.fit(trainX, trainY, epochs=n_epoch, shuffle=True, batch_size=batch_size, validation_split= 0.05,\n",
    "                  verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = blstm_atten(trainX.shape[1], vocab_size, embed_size, num_classes, learn_rate)\n",
    "model.fit(trainX, trainY, epochs=n_epoch, shuffle=True, batch_size=batch_size, validation_split= 0.05,\n",
    "                  verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = model.predict(testX)\n",
    "y_pred  = np.argmax(temp, 1)\n",
    "y_true = np.argmax(testY, 1)\n",
    "precision = metrics.precision_score(y_true, y_pred, average=None)\n",
    "recall = metrics.recall_score(y_true, y_pred, average=None)\n",
    "f1_score = metrics.f1_score(y_true, y_pred, average=None)\n",
    "print(\"Precision: \" + str(precision) + \"\\n\")\n",
    "print(\"Recall: \" + str(recall) + \"\\n\")\n",
    "print(\"f1_score: \" + str(f1_score) + \"\\n\")\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "print(\":: Classification Report\")\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_processor.save(\"Models/SARC_vocab_glove.pkl\")\n",
    "model.save(\"Models/sarc_cnn_glove.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model(\"Models/sarc_model_glove.h5\", custom_objects={'AttLayer':AttLayer})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transfer Learning Variant I -> Model All parameters transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(trainX, trainY, epochs=n_epoch, shuffle=True, batch_size=batch_size, validation_split= 0.05, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transfer Learning Variant II -> Embedding parameters transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = vocab._mapping\n",
    "trained_model = load_model(\"Models/sarc_model_glove.h5\", custom_objects={'AttLayer':AttLayer})\n",
    "embedding = trained_model.layers[0].get_weights()\n",
    "model = blstm_atten(trainX.shape[1], vocab_size, embed_size, num_classes, learn_rate)\n",
    "model.layers[0].set_weights(embedding)\n",
    "model.fit(trainX, trainY, epochs=n_epoch, shuffle=True, batch_size=batch_size, validation_split= 0.05,\n",
    "                  verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transfer Learning Variant III -> Direct test the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = model.predict(testX)\n",
    "y_pred  = np.argmax(temp, 1)\n",
    "y_true = np.argmax(testY, 1)\n",
    "precision = metrics.precision_score(y_true, y_pred, average=None)\n",
    "recall = metrics.recall_score(y_true, y_pred, average=None)\n",
    "f1_score = metrics.f1_score(y_true, y_pred, average=None)\n",
    "print(\"Precision: \" + str(precision) + \"\\n\")\n",
    "print(\"Recall: \" + str(recall) + \"\\n\")\n",
    "print(\"f1_score: \" + str(f1_score) + \"\\n\")\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "print(\":: Classification Report\")\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_with_labels(low_dim_embs, labels, filename='tsne.png'):\n",
    "    assert low_dim_embs.shape[0] >= len(labels), 'More labels than embeddings'\n",
    "    plt.figure(figsize=(14, 14))  # in inches\n",
    "    for i, label in enumerate(labels):\n",
    "        x, y = low_dim_embs[i, :]\n",
    "        plt.scatter(x, y)\n",
    "        plt.annotate(label,\n",
    "                     xy=(x, y),\n",
    "                     xytext=(5, 2),\n",
    "                     textcoords='offset points',\n",
    "                     ha='right',\n",
    "                     va='bottom')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_embedding(embedding, reverse_dictionary):\n",
    "    tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=2000)\n",
    "    low_dim_embs = tsne.fit_transform(embedding)\n",
    "    labels = [reverse_dictionary[i] for i in xrange(vocab_size)]\n",
    "    plot_with_labels(low_dim_embs, labels)\n",
    "    return low_dim_embs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reverse_dict = vocab_processor.vocabulary_._reverse_mapping\n",
    "embedding = model.layers[0].get_weights()[0]\n",
    "low_dim_embs = plot_embedding(embedding, reverse_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.utils import shuffle\n",
    "from string import punctuation\n",
    "import re\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import make_scorer, f1_score, accuracy_score, recall_score, precision_score, classification_report, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_data(x_text, labels):\n",
    "    x_text = np.array(list(vocab_processor.transform(x_text)))\n",
    "    embedding_weights = model.layers[0].get_weights()[0]\n",
    "    X, y = [], []\n",
    "    for i in range(len(x_text)):\n",
    "        emb = np.zeros(embed_size)\n",
    "        for word in x_text[i]:\n",
    "            try:\n",
    "                emb += embedding_weights[word]\n",
    "            except:\n",
    "                print \"Here\"\n",
    "                pass\n",
    "        emb /= len(x_text[i])\n",
    "        X.append(emb)\n",
    "        y.append(labels[i])\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainX, trainY = gen_data(X_train, Y_train)\n",
    "testX, testY = gen_data(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savez('transferSARC',indices,trainX,testX,trainY,testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = xgb.XGBClassifier()\n",
    "logreg.fit(trainX, trainY)\n",
    "\n",
    "y_pred = logreg.predict(testX)\n",
    "y_true = Y_test\n",
    "precision = metrics.precision_score(y_true, y_pred, average=None)\n",
    "recall = metrics.recall_score(y_true, y_pred, average=None)\n",
    "f1_score = metrics.f1_score(y_true, y_pred, average=None)\n",
    "print(\"Precision: \" + str(precision) + \"\\n\")\n",
    "print(\"Recall: \" + str(recall) + \"\\n\")\n",
    "print(\"f1_score: \" + str(f1_score) + \"\\n\")\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "print(\":: Classification Report\")\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classification_model(X, Y):\n",
    "    NO_OF_FOLDS=10\n",
    "    X, Y = shuffle(X, Y, random_state=42)\n",
    "    logreg = xgb.XGBClassifier()\n",
    "    scores2 = cross_val_score(logreg, X, Y, cv=NO_OF_FOLDS, scoring='recall_weighted')\n",
    "    print \"Recall(avg): %0.3f (+/- %0.3f)\" % (scores2.mean(), scores2.std() * 2)\n",
    "    scores1 = cross_val_score(logreg, X, Y, cv=NO_OF_FOLDS, scoring='precision_weighted')\n",
    "    print \"Precision(avg): %0.3f (+/- %0.3f)\" % (scores1.mean(), scores1.std() * 2)    \n",
    "    scores3 = cross_val_score(logreg, X, Y, cv=NO_OF_FOLDS, scoring='f1_weighted')\n",
    "    print \"F1-score(avg): %0.3f (+/- %0.3f)\" % (scores3.mean(), scores3.std() * 2)\n",
    "    print(scores1, scores2, scores3)\n",
    "\n",
    "X, Y = gen_data(x_text, labels)\n",
    "classification_model(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
