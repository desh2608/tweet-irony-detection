{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import preprocessor as p\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "import preprocessor as p\n",
    "from collections import Counter\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix \n",
    "from tensorflow.contrib import learn\n",
    "from tflearn.data_utils import to_categorical, pad_sequences\n",
    "import os\n",
    "os.environ['KERAS_BACKEND']='theano'\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Merge, Dropout, LSTM, GRU, Bidirectional, Activation\n",
    "from keras.models import Model,Sequential\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras import initializers, optimizers\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D, SpatialDropout1D\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from keras.regularizers import L1L2\n",
    "from keras.layers.merge import concatenate\n",
    "import xgboost as xgb\n",
    "from sklearn.utils import shuffle\n",
    "from string import punctuation\n",
    "import re\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import make_scorer, f1_score, accuracy_score, recall_score, precision_score, classification_report, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Irony Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_hashtags = {}\n",
    "def get_hashtags(tweet):\n",
    "    parsed_tweet = p.parse(tweet.decode('ascii', 'ignore').encode('ascii').lower())\n",
    "    parsed_hashtags = parsed_tweet.hashtags\n",
    "    \n",
    "    hashtags = []\n",
    "    if parsed_hashtags is not None:\n",
    "        for hashtag in parsed_hashtags:\n",
    "            temp = hashtag.match[1:].lower()\n",
    "            if temp in all_hashtags:\n",
    "                all_hashtags[temp] += 1\n",
    "            else:\n",
    "                all_hashtags[temp] = 1\n",
    "            hashtags.append(temp)\n",
    "\n",
    "    hashtags_str = (\" \").join(hashtags)\n",
    "    return hashtags_str, len(hashtags)\n",
    "\n",
    "def get_clean_tweet(tweet):\n",
    "    p.set_options(p.OPT.URL)\n",
    "    clean_tweet = p.clean(tweet)\n",
    "    return clean_tweet.lower().replace(\"#\",\" \")\n",
    "\n",
    "\n",
    "emotion_keys = {}\n",
    "def get_emotion(tweet):\n",
    "    result = re.findall(r\":\\w+_\\w+:\",tweet)\n",
    "    if result is not None:\n",
    "        emotions = []\n",
    "        for i in range(len(result)):\n",
    "            emotion = result[i][1:-1]\n",
    "            emotions.append(emotion)\n",
    "            if emotion in emotion_keys:\n",
    "                emotion_keys[emotion] += 1\n",
    "            else:\n",
    "                emotion_keys[emotion] = 1\n",
    "    return (\" \").join(emotions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data(filename):\n",
    "    \n",
    "    data = pd.DataFrame()\n",
    "    \n",
    "    tweets = []\n",
    "    labels = []\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            _, label, tweet = line.strip().split(\"\\t\")\n",
    "            if(count)==0:\n",
    "                count+=1\n",
    "                continue\n",
    "            tweets.append(tweet)\n",
    "            labels.append(label)\n",
    "            count+=1\n",
    "            \n",
    "    print \"Lines read: \" + str(count)\n",
    "    data[\"Tweet text\"] = tweets\n",
    "    data[\"Label\"] = labels\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = get_data(\"./datasets/train/SemEval2018-T3-train-taskA.txt\")\n",
    "data['hashtags'], data['length'] = zip(*data['Tweet text'].map(get_hashtags)) \n",
    "data[\"tweet\"] = data['Tweet text'].map(get_clean_tweet)\n",
    "data['emotion'] = data['tweet'].map(get_emotion)\n",
    "data.head()\n",
    "data.to_pickle(\"Irony_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_pickle(\"Irony_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_text = np.array(data[\"tweet\"].tolist())\n",
    "label =  np.array(data[\"Label\"].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomize indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indices = np.arange(x_text.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "indices.dump(\"split.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using word and character unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "print(\"Using char n-grams based features\")\n",
    "bow_transformer = CountVectorizer(max_features = 10000, ngram_range = (1,5)).fit(x_text)\n",
    "comments_bow = bow_transformer.transform(x_text)\n",
    "tfidf_transformer = TfidfTransformer(norm = 'l2').fit(comments_bow)\n",
    "comments_tfidf = tfidf_transformer.transform(comments_bow)\n",
    "features = comments_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "import scipy\n",
    "#Irony\n",
    "VALIDATION_SPLIT = 0.2\n",
    "indices = np.load(\"split.pkl\")\n",
    "features = features[indices]\n",
    "label = label[indices]\n",
    "num_validation_samples = int(VALIDATION_SPLIT * features.shape[0])\n",
    "\n",
    "X_train = scipy.sparse.csr_matrix.todense(features[:-num_validation_samples])\n",
    "Y_train = label[:-num_validation_samples]\n",
    "X_test = scipy.sparse.csr_matrix.todense(features[-num_validation_samples:])\n",
    "Y_test = label[-num_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(class_weight=\"balanced\")\n",
    "logreg.fit(X_train, Y_train)\n",
    "\n",
    "y_pred = logreg.predict(X_test)\n",
    "y_true = Y_test\n",
    "precision = metrics.precision_score(y_true, y_pred, average=None)\n",
    "recall = metrics.recall_score(y_true, y_pred, average=None)\n",
    "f1_score = metrics.f1_score(y_true, y_pred, average=None)\n",
    "accuracy = metrics.accuracy_score(y_true, y_pred)\n",
    "print(\"Precision: \" + str(precision) + \"\\n\")\n",
    "print(\"Recall: \" + str(recall) + \"\\n\")\n",
    "print(\"f1_score: \" + str(f1_score) + \"\\n\")\n",
    "print(\"Accuracy: \" + str(accuracy) + \"\\n\")\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "print(\":: Classification Report\")\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savez('trainedmodels/char_ngrams_count',indices, X_train, X_test, Y_train, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load SARC data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "f = open(\"./datasets/SARC/data_without_ancestor.pkl\", \"r\")\n",
    "x_text = pickle.load(f)\n",
    "label = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Deep Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_embedding_weights(filename, sep):\n",
    "    embed_dict = {}\n",
    "    file = open(filename,'r')\n",
    "    for line in file.readlines():\n",
    "        row = line.strip().split(sep)\n",
    "        embed_dict[row[0]] = row[1:]\n",
    "    print('Loaded from file: ' + str(filename))\n",
    "    file.close()\n",
    "    return embed_dict\n",
    "\n",
    "def map_embedding_weights(embed, vocab, embed_size):\n",
    "    vocab_size = len(vocab)\n",
    "    embeddingWeights = np.zeros((vocab_size , embed_size))\n",
    "    n = 0\n",
    "    words_missed = []\n",
    "    for k, v in vocab.iteritems():\n",
    "        try:\n",
    "            embeddingWeights[v] = embed[k]\n",
    "        except:\n",
    "            n += 1\n",
    "            words_missed.append(k)\n",
    "            pass\n",
    "    print(\"%d embedding missed\"%n, \" of \" , vocab_size)\n",
    "    return embeddingWeights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AttLayer(Layer):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Create a trainable weight variable for this layer.\n",
    "        self.W = self.add_weight(name='kernel', \n",
    "                                      shape=(input_shape[-1],),\n",
    "                                      initializer='random_normal',\n",
    "                                      trainable=True)\n",
    "        super(AttLayer, self).build(input_shape)  # Be sure to call this somewhere!\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        eij = K.tanh(K.dot(x, self.W))\n",
    "        \n",
    "        ai = K.exp(eij)\n",
    "        weights = ai/K.sum(ai, axis=1).dimshuffle(0,'x')\n",
    "        \n",
    "        weighted_input = x*weights.dimshuffle(0,1,'x')\n",
    "        return weighted_input.sum(axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])\n",
    "\n",
    "def blstm_atten(inp_dim, vocab_size, embed_size, num_classes, learn_rate):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, embed_size, input_length=inp_dim))\n",
    "    model.add(SpatialDropout1D(0.25))\n",
    "    model.add(Bidirectional(LSTM(embed_size, return_sequences=True)))\n",
    "    model.add(AttLayer())\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "def blstm_atten_2(inp_dim, vocab_size, embed_size, num_classes, learn_rate):\n",
    "    model_input = Input(shape=(inp_dim,), dtype='int32')\n",
    "    embed = Embedding(vocab_size, embed_size, input_length=inp_dim)(model_input)\n",
    "    embed_drop = Dropout(0.25)(embed)\n",
    "    lstm_output_0 = Bidirectional(LSTM(embed_size, return_sequences=True), name=\"bi_lstm_0\")(embed_drop)\n",
    "    lstm_output_1 = Bidirectional(LSTM(embed_size, return_sequences=True), name=\"bi_lstm_1\")(lstm_output_0)\n",
    "    x = concatenate([embed, lstm_output_0, lstm_output_1])\n",
    "    atten_output = AttLayer()(x)\n",
    "    drop =  Dropout(0.25)(atten_output)\n",
    "    output = Dense(num_classes, activation='softmax')(drop)\n",
    "    model = Model(inputs=[model_input], outputs=output, name=\"Irony\")\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "def cnn_lstm(inp_dim, vocab_size, embed_size, num_classes, learn_rate):\n",
    "    # Convolution\n",
    "    kernel_size = 5\n",
    "    filters = 64\n",
    "    pool_size = 4\n",
    "\n",
    "    # LSTM\n",
    "    lstm_output_size = 70\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, embed_size, input_length=inp_dim))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv1D(filters,\n",
    "                     kernel_size,\n",
    "                     padding='valid',\n",
    "                     activation='relu',\n",
    "                     strides=1))\n",
    "    model.add(MaxPooling1D(pool_size=pool_size))\n",
    "    model.add(LSTM(lstm_output_size))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def blstm(inp_dim,vocab_size, embed_size, num_classes, learn_rate):   \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, embed_size, input_length=inp_dim, trainable=True))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Bidirectional(LSTM(embed_size)))\n",
    "    model.add(Dropout(0.50))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Settings for Irony Data\n",
    "max_document_length = 50\n",
    "num_classes = 2\n",
    "embed_size = 50\n",
    "n_epoch = 10\n",
    "batch_size = 16\n",
    "learn_rate = 0.01\n",
    "max_features = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Settings for SARC Data\n",
    "max_document_length = 50\n",
    "num_classes = 2\n",
    "embed_size = 50\n",
    "n_epoch = 15\n",
    "batch_size = 256\n",
    "learn_rate = 0.01\n",
    "max_features = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#SARC\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(x_text, label, random_state=42, test_size=0.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Irony\n",
    "VALIDATION_SPLIT = 0.2\n",
    "indices = np.load(\"split.pkl\")\n",
    "x_text = x_text[indices]\n",
    "label = label[indices]\n",
    "num_validation_samples = int(VALIDATION_SPLIT * x_text.shape[0])\n",
    "\n",
    "X_train = x_text[:-num_validation_samples]\n",
    "Y_train = label[:-num_validation_samples]\n",
    "X_test = x_text[-num_validation_samples:]\n",
    "Y_test = label[-num_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length, max_features)\n",
    "# vocab_processor = vocab_processor.fit(x_text)\n",
    "vocab_processor = vocab_processor.restore(\"./Models/SARC_vocab_glove.pkl\")\n",
    "\n",
    "vocab_size = len(vocab_processor.vocabulary_._mapping)\n",
    "print(\"Vocabulary Size: {:d}\".format(vocab_size))\n",
    "vocab = vocab_processor.vocabulary_\n",
    "\n",
    "\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length, max_features, vocabulary=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainX = np.array(list(vocab_processor.transform(X_train)))\n",
    "testX = np.array(list(vocab_processor.transform(X_test)))\n",
    "\n",
    "trainY = np.asarray(Y_train)\n",
    "testY = np.asarray(Y_test)\n",
    "\n",
    "trainX = pad_sequences(trainX, maxlen=max_document_length, value=0.)\n",
    "testX = pad_sequences(testX, maxlen=max_document_length, value=0.)\n",
    "\n",
    "trainY = to_categorical(trainY, nb_classes=num_classes)\n",
    "testY = to_categorical(testY, nb_classes=num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = \"word_vectors/glove.twitter.27B.50d.txt\"\n",
    "sep = \" \"\n",
    "vocab = vocab_processor.vocabulary_._mapping\n",
    "model = blstm_atten_2(trainX.shape[1], vocab_size, embed_size, num_classes, learn_rate)\n",
    "model.layers[1].set_weights([map_embedding_weights(get_embedding_weights(filename, sep), vocab, embed_size)])\n",
    "model.fit(trainX, trainY, epochs=n_epoch, shuffle=True, batch_size=batch_size, validation_split= 0.05,\n",
    "                  verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = blstm_atten_2(trainX.shape[1], vocab_size, embed_size, num_classes, learn_rate)\n",
    "model.fit(trainX, trainY, epochs=n_epoch, shuffle=True, batch_size=batch_size, validation_split= 0.05,verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp = model.predict(testX)\n",
    "y_pred  = np.argmax(temp, 1)\n",
    "y_true = np.argmax(testY, 1)\n",
    "precision = metrics.precision_score(y_true, y_pred, average=None)\n",
    "recall = metrics.recall_score(y_true, y_pred, average=None)\n",
    "f1_score = metrics.f1_score(y_true, y_pred, average=None)\n",
    "print(\"Precision: \" + str(precision) + \"\\n\")\n",
    "print(\"Recall: \" + str(recall) + \"\\n\")\n",
    "print(\"f1_score: \" + str(f1_score) + \"\\n\")\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "print(\":: Classification Report\")\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_processor.save(\"Models/SARC_vocab_glove.pkl\")\n",
    "model.save(\"Models/sarc_cnn_glove.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model(\"Models/sarc_model_glove.h5\", custom_objects={'AttLayer':AttLayer})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transfer Learning Variant I -> Model All parameters transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.fit(trainX, trainY, epochs=n_epoch, shuffle=True, batch_size=batch_size, validation_split= 0.05, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save(\"trainedmodels/sarc_transfer_all.h5\")\n",
    "np.savez('trainedmodels/sarc_transfer_all',indices,trainX,testX,trainY,testY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transfer Learning Variant II -> Embedding parameters transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = vocab._mapping\n",
    "trained_model = load_model(\"Models/sarc_model_glove.h5\", custom_objects={'AttLayer':AttLayer})\n",
    "embedding = trained_model.layers[0].get_weights()\n",
    "model = blstm_atten(trainX.shape[1], vocab_size, embed_size, num_classes, learn_rate)\n",
    "model.layers[0].set_weights(embedding)\n",
    "model.fit(trainX, trainY, epochs=n_epoch, shuffle=True, batch_size=batch_size, validation_split= 0.05,\n",
    "                  verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transfer Learning Variant III -> Direct test the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp = model.predict(testX)\n",
    "y_pred  = np.argmax(temp, 1)\n",
    "y_true = np.argmax(testY, 1)\n",
    "precision = metrics.precision_score(y_true, y_pred, average=None)\n",
    "recall = metrics.recall_score(y_true, y_pred, average=None)\n",
    "f1_score = metrics.f1_score(y_true, y_pred, average=None)\n",
    "print(\"Precision: \" + str(precision) + \"\\n\")\n",
    "print(\"Recall: \" + str(recall) + \"\\n\")\n",
    "print(\"f1_score: \" + str(f1_score) + \"\\n\")\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "print(\":: Classification Report\")\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_with_labels(low_dim_embs, labels, filename='tsne.png'):\n",
    "    assert low_dim_embs.shape[0] >= len(labels), 'More labels than embeddings'\n",
    "    plt.figure(figsize=(14, 14))  # in inches\n",
    "    for i, label in enumerate(labels):\n",
    "        x, y = low_dim_embs[i, :]\n",
    "        plt.scatter(x, y)\n",
    "        plt.annotate(label,\n",
    "                     xy=(x, y),\n",
    "                     xytext=(5, 2),\n",
    "                     textcoords='offset points',\n",
    "                     ha='right',\n",
    "                     va='bottom')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_embedding(embedding, reverse_dictionary):\n",
    "    tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=2000)\n",
    "    low_dim_embs = tsne.fit_transform(embedding)\n",
    "    labels = [reverse_dictionary[i] for i in xrange(vocab_size)]\n",
    "    plot_with_labels(low_dim_embs, labels)\n",
    "    return low_dim_embs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reverse_dict = vocab_processor.vocabulary_._reverse_mapping\n",
    "embedding = model.layers[0].get_weights()[0]\n",
    "low_dim_embs = plot_embedding(embedding, reverse_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_data(x_text, labels):\n",
    "    x_text = np.array(list(vocab_processor.transform(x_text)))\n",
    "    embedding_weights = model.layers[0].get_weights()[0]\n",
    "    X, y = [], []\n",
    "    for i in range(len(x_text)):\n",
    "        emb = np.zeros(embed_size)\n",
    "        for word in x_text[i]:\n",
    "            try:\n",
    "                emb += embedding_weights[word]\n",
    "            except:\n",
    "                print \"Here\"\n",
    "                pass\n",
    "        emb /= len(x_text[i])\n",
    "        X.append(emb)\n",
    "        y.append(labels[i])\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainX, trainY = gen_data(X_train, Y_train)\n",
    "testX, testY = gen_data(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logreg = xgb.XGBClassifier()\n",
    "logreg.fit(trainX, trainY)\n",
    "\n",
    "y_pred = logreg.predict(testX)\n",
    "y_true = Y_test\n",
    "precision = metrics.precision_score(y_true, y_pred, average=None)\n",
    "recall = metrics.recall_score(y_true, y_pred, average=None)\n",
    "f1_score = metrics.f1_score(y_true, y_pred, average=None)\n",
    "print(\"Precision: \" + str(precision) + \"\\n\")\n",
    "print(\"Recall: \" + str(recall) + \"\\n\")\n",
    "print(\"f1_score: \" + str(f1_score) + \"\\n\")\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "print(\":: Classification Report\")\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classification_model(X, Y):\n",
    "    NO_OF_FOLDS=10\n",
    "    X, Y = shuffle(X, Y, random_state=42)\n",
    "    logreg = xgb.XGBClassifier()\n",
    "    scores2 = cross_val_score(logreg, X, Y, cv=NO_OF_FOLDS, scoring='recall_weighted')\n",
    "    print \"Recall(avg): %0.3f (+/- %0.3f)\" % (scores2.mean(), scores2.std() * 2)\n",
    "    scores1 = cross_val_score(logreg, X, Y, cv=NO_OF_FOLDS, scoring='precision_weighted')\n",
    "    print \"Precision(avg): %0.3f (+/- %0.3f)\" % (scores1.mean(), scores1.std() * 2)    \n",
    "    scores3 = cross_val_score(logreg, X, Y, cv=NO_OF_FOLDS, scoring='f1_weighted')\n",
    "    print \"F1-score(avg): %0.3f (+/- %0.3f)\" % (scores3.mean(), scores3.std() * 2)\n",
    "    print(scores1, scores2, scores3)\n",
    "\n",
    "X, Y = gen_data(x_text, labels)\n",
    "classification_model(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DeepMoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Settings for SARC Data\n",
    "max_document_length = 50\n",
    "#Irony\n",
    "VALIDATION_SPLIT = 0.2\n",
    "indices = np.load(\"split.pkl\")\n",
    "x_text = x_text[indices]\n",
    "label = label[indices]\n",
    "num_validation_samples = int(VALIDATION_SPLIT * x_text.shape[0])\n",
    "\n",
    "X_train = x_text[:-num_validation_samples]\n",
    "Y_train = label[:-num_validation_samples]\n",
    "X_test = x_text[-num_validation_samples:]\n",
    "Y_test = label[-num_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"DeepMoji\")\n",
    "sys.path.append(\"/Library/Python/2.7/site-packages\")\n",
    "# import example_helper\n",
    "import json\n",
    "import csv\n",
    "import numpy as np\n",
    "from deepmoji.sentence_tokenizer import SentenceTokenizer\n",
    "from deepmoji.model_def import deepmoji_feature_encoding\n",
    "from deepmoji.global_variables import PRETRAINED_PATH, VOCAB_PATH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_encoded_deepmoji(x_text, labels):\n",
    "    \n",
    "    # Decode data\n",
    "    try:\n",
    "        x_text = [unicode(x) for x in x_text]\n",
    "    except UnicodeDecodeError:\n",
    "        x_text = [x.decode('utf-8') for x in x_text]\n",
    "    \n",
    "    print('Tokenizing using dictionary from {}'.format(VOCAB_PATH))\n",
    "    with open(VOCAB_PATH, 'r') as f:\n",
    "        vocabulary = json.load(f)\n",
    "    st = SentenceTokenizer(vocabulary, max_document_length)\n",
    "    tokenized, _, _ = st.tokenize_sentences(x_text)\n",
    "\n",
    "    print('Loading model from {}.'.format(PRETRAINED_PATH))\n",
    "    model = deepmoji_feature_encoding(max_document_length, PRETRAINED_PATH)\n",
    "    model.summary()\n",
    "\n",
    "    print('Encoding texts..')\n",
    "    encoding = model.predict(tokenized)\n",
    "    \n",
    "    X = np.array(encoding)\n",
    "    y = np.array(labels)\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainX, trainY = get_encoded_deepmoji(X_train, Y_train)\n",
    "testX, testY = get_encoded_deepmoji(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logreg = xgb.XGBClassifier()\n",
    "logreg.fit(trainX, trainY)\n",
    "\n",
    "y_pred = logreg.predict(testX)\n",
    "y_true = Y_test\n",
    "precision = metrics.precision_score(y_true, y_pred, average=None)\n",
    "recall = metrics.recall_score(y_true, y_pred, average=None)\n",
    "f1_score = metrics.f1_score(y_true, y_pred, average=None)\n",
    "print(\"Precision: \" + str(precision) + \"\\n\")\n",
    "print(\"Recall: \" + str(recall) + \"\\n\")\n",
    "print(\"f1_score: \" + str(f1_score) + \"\\n\")\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "print(\":: Classification Report\")\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savez('trainedmodels/deepmoji',indices,trainX,testX,trainY,testY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Sentiment base features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()\n",
    "def get_sentiment_score(tweet):\n",
    "    ss = sid.polarity_scores(tweet)\n",
    "    return np.array([ss[\"pos\"], ss[\"neg\"], ss[\"neu\"]])\n",
    "\n",
    "def get_sentiment(x_text, labels):\n",
    "    encoding = []\n",
    "    for tweet in x_text:\n",
    "        encoding.append(get_sentiment_score(tweet))\n",
    "    X = np.array(encoding)\n",
    "    y = np.array(labels)\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainX, trainY = get_sentiment(X_train, Y_train)\n",
    "testX, testY = get_sentiment(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savez('trainedmodels/sent_features',indices, trainX, testX, trainY, testY)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
