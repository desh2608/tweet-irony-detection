{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import preprocessor as p\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "import preprocessor as p\n",
    "from collections import Counter\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix \n",
    "from tensorflow.contrib import learn\n",
    "from tflearn.data_utils import to_categorical, pad_sequences\n",
    "import os\n",
    "os.environ['KERAS_BACKEND']='theano'\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Merge, Dropout, LSTM, GRU, Bidirectional\n",
    "from keras.models import Model,Sequential\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras import initializers, optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./datasets/train/SemEval2018-T3-train-taskA.txt\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_hashtags = {}\n",
    "def get_hashtags(tweet):\n",
    "    parsed_tweet = p.parse(tweet.decode('ascii', 'ignore').encode('ascii').lower())\n",
    "    parsed_hashtags = parsed_tweet.hashtags\n",
    "    \n",
    "    hashtags = []\n",
    "    if parsed_hashtags is not None:\n",
    "        for hashtag in parsed_hashtags:\n",
    "            temp = hashtag.match[1:].lower()\n",
    "            if temp in all_hashtags:\n",
    "                all_hashtags[temp] += 1\n",
    "            else:\n",
    "                all_hashtags[temp] = 1\n",
    "            hashtags.append(temp)\n",
    "\n",
    "    hashtags_str = (\" \").join(hashtags)\n",
    "    return hashtags_str, len(hashtags)\n",
    "\n",
    "def get_clean_tweet(tweet):\n",
    "    p.set_options(p.OPT.URL)\n",
    "    clean_tweet = p.clean(tweet)\n",
    "    return clean_tweet.lower().replace(\"#\",\" \")\n",
    "\n",
    "\n",
    "emotion_keys = {}\n",
    "def get_emotion(tweet):\n",
    "    result = re.findall(r\":\\w+_\\w+:\",tweet)\n",
    "    if result is not None:\n",
    "        emotions = []\n",
    "        for i in range(len(result)):\n",
    "            emotion = result[i][1:-1]\n",
    "            emotions.append(emotion)\n",
    "            if emotion in emotion_keys:\n",
    "                emotion_keys[emotion] += 1\n",
    "            else:\n",
    "                emotion_keys[emotion] = 1\n",
    "    return (\" \").join(emotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet index</th>\n",
       "      <th>Label</th>\n",
       "      <th>Tweet text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>length</th>\n",
       "      <th>tweet</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Sweet United Nations video. Just in time for Christmas. #imagine #NoReligion  http://t.co/fej2v3OUBR</td>\n",
       "      <td>imagine noreligion</td>\n",
       "      <td>2</td>\n",
       "      <td>sweet united nations video. just in time for christmas.  imagine  noreligion</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>@mrdahl87 We are rumored to have talked to Erv's agent... and the Angels asked about Ed Escobar... that's hardly nothing    ;)</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>@mrdahl87 we are rumored to have talked to erv's agent... and the angels asked about ed escobar... that's hardly nothing ;)</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Hey there! Nice to see you Minnesota/ND Winter Weather</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>hey there! nice to see you minnesota/nd winter weather</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3 episodes left I'm dying over here</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>3 episodes left i'm dying over here</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>I can't breathe! was chosen as the most notable quote of the year in an annual list released by a Yale University librarian</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>i can't breathe! was chosen as the most notable quote of the year in an annual list released by a yale university librarian</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Tweet index  Label  \\\n",
       "0  1            1       \n",
       "1  2            1       \n",
       "2  3            1       \n",
       "3  4            0       \n",
       "4  5            1       \n",
       "\n",
       "                                                                                                                       Tweet text  \\\n",
       "0  Sweet United Nations video. Just in time for Christmas. #imagine #NoReligion  http://t.co/fej2v3OUBR                             \n",
       "1  @mrdahl87 We are rumored to have talked to Erv's agent... and the Angels asked about Ed Escobar... that's hardly nothing    ;)   \n",
       "2  Hey there! Nice to see you Minnesota/ND Winter Weather                                                                           \n",
       "3  3 episodes left I'm dying over here                                                                                              \n",
       "4  I can't breathe! was chosen as the most notable quote of the year in an annual list released by a Yale University librarian      \n",
       "\n",
       "             hashtags  length  \\\n",
       "0  imagine noreligion  2        \n",
       "1                      0        \n",
       "2                      0        \n",
       "3                      0        \n",
       "4                      0        \n",
       "\n",
       "                                                                                                                         tweet  \\\n",
       "0  sweet united nations video. just in time for christmas.  imagine  noreligion                                                  \n",
       "1  @mrdahl87 we are rumored to have talked to erv's agent... and the angels asked about ed escobar... that's hardly nothing ;)   \n",
       "2  hey there! nice to see you minnesota/nd winter weather                                                                        \n",
       "3  3 episodes left i'm dying over here                                                                                           \n",
       "4  i can't breathe! was chosen as the most notable quote of the year in an annual list released by a yale university librarian   \n",
       "\n",
       "  emotion  \n",
       "0          \n",
       "1          \n",
       "2          \n",
       "3          \n",
       "4          "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['hashtags'], data['length'] = zip(*data['Tweet text'].map(get_hashtags)) \n",
    "data[\"tweet\"] = data['Tweet text'].map(get_clean_tweet)\n",
    "data['emotion'] = data['tweet'].map(get_emotion)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttLayer(Layer):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Create a trainable weight variable for this layer.\n",
    "        self.W = self.add_weight(name='kernel', \n",
    "                                      shape=(input_shape[-1],),\n",
    "                                      initializer='random_normal',\n",
    "                                      trainable=True)\n",
    "        super(AttLayer, self).build(input_shape)  # Be sure to call this somewhere!\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        eij = K.tanh(K.dot(x, self.W))\n",
    "        \n",
    "        ai = K.exp(eij)\n",
    "        weights = ai/K.sum(ai, axis=1).dimshuffle(0,'x')\n",
    "        \n",
    "        weighted_input = x*weights.dimshuffle(0,1,'x')\n",
    "        return weighted_input.sum(axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])\n",
    "\n",
    "def blstm_atten(inp_dim, vocab_size, embed_size, num_classes, learn_rate):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, embed_size, input_length=inp_dim))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Bidirectional(LSTM(embed_size, return_sequences=True)))\n",
    "    model.add(AttLayer())\n",
    "    model.add(Dropout(0.50))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    adam = optimizers.Adam(lr=learn_rate, beta_1=0.9, beta_2=0.999)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_document_length = 38\n",
    "num_classes = 2\n",
    "embed_size = 50\n",
    "n_epoch = 10\n",
    "batch_size = 16\n",
    "learn_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 2315\n"
     ]
    }
   ],
   "source": [
    "x_text = data[\"tweet\"].tolist()\n",
    "labels =  data[\"Label\"].tolist()\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(x_text, labels, random_state=121, test_size=0.10)\n",
    "\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length, 2)\n",
    "vocab_processor = vocab_processor.fit(x_text)\n",
    "\n",
    "vocab_size = len(vocab_processor.vocabulary_)\n",
    "print(\"Vocabulary Size: {:d}\".format(vocab_size))\n",
    "vocab = vocab_processor.vocabulary_._mapping\n",
    "\n",
    "trainX = np.array(list(vocab_processor.transform(X_train)))\n",
    "testX = np.array(list(vocab_processor.transform(X_test)))\n",
    "\n",
    "trainY = np.asarray(Y_train)\n",
    "testY = np.asarray(Y_test)\n",
    "\n",
    "trainX = pad_sequences(trainX, maxlen=max_document_length, value=0.)\n",
    "testX = pad_sequences(testX, maxlen=max_document_length, value=0.)\n",
    "\n",
    "trainY = to_categorical(trainY, nb_classes=num_classes)\n",
    "testY = to_categorical(testY, nb_classes=num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 38, 50)            115750    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 38, 50)            0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 38, 100)           40400     \n",
      "_________________________________________________________________\n",
      "att_layer_1 (AttLayer)       (None, 100)               100       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 156,452\n",
      "Trainable params: 156,452\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "3435/3435 [==============================] - 5s - loss: 0.6932 - acc: 0.5127     \n",
      "Epoch 2/10\n",
      "3435/3435 [==============================] - 5s - loss: 0.6252 - acc: 0.6480     \n",
      "Epoch 3/10\n",
      "3435/3435 [==============================] - 5s - loss: 0.5052 - acc: 0.7613     \n",
      "Epoch 4/10\n",
      "3435/3435 [==============================] - 5s - loss: 0.4336 - acc: 0.8052     \n",
      "Epoch 5/10\n",
      "3435/3435 [==============================] - 5s - loss: 0.3727 - acc: 0.8390     \n",
      "Epoch 6/10\n",
      "3435/3435 [==============================] - 5s - loss: 0.3144 - acc: 0.8635     \n",
      "Epoch 7/10\n",
      "3435/3435 [==============================] - 5s - loss: 0.2883 - acc: 0.8766     \n",
      "Epoch 8/10\n",
      "3435/3435 [==============================] - 5s - loss: 0.2483 - acc: 0.8955     \n",
      "Epoch 9/10\n",
      "3435/3435 [==============================] - 5s - loss: 0.2320 - acc: 0.8929     \n",
      "Epoch 10/10\n",
      "3435/3435 [==============================] - 5s - loss: 0.2157 - acc: 0.9019     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x121d51e50>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = blstm_atten(trainX.shape[1], vocab_size, embed_size, num_classes, learn_rate)\n",
    "model.fit(trainX, trainY, epochs=n_epoch, shuffle=True, batch_size=batch_size, \n",
    "                  verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: [ 0.6127451   0.62921348]\n",
      "\n",
      "Recall: [ 0.65445026  0.58638743]\n",
      "\n",
      "f1_score: [ 0.63291139  0.60704607]\n",
      "\n",
      "[[125  66]\n",
      " [ 79 112]]\n",
      ":: Classification Report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.61      0.65      0.63       191\n",
      "          1       0.63      0.59      0.61       191\n",
      "\n",
      "avg / total       0.62      0.62      0.62       382\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temp = model.predict(testX)\n",
    "y_pred  = np.argmax(temp, 1)\n",
    "y_true = np.argmax(testY, 1)\n",
    "precision = metrics.precision_score(y_true, y_pred, average=None)\n",
    "recall = metrics.recall_score(y_true, y_pred, average=None)\n",
    "f1_score = metrics.f1_score(y_true, y_pred, average=None)\n",
    "print(\"Precision: \" + str(precision) + \"\\n\")\n",
    "print(\"Recall: \" + str(recall) + \"\\n\")\n",
    "print(\"f1_score: \" + str(f1_score) + \"\\n\")\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "print(\":: Classification Report\")\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.utils import shuffle\n",
    "from string import punctuation\n",
    "import re\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import make_scorer, f1_score, accuracy_score, recall_score, precision_score, classification_report, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_data(x_text, labels):\n",
    "    x_text = np.array(list(vocab_processor.transform(x_text)))\n",
    "    embedding_weights = model.layers[0].get_weights()[0]\n",
    "    X, y = [], []\n",
    "    for i in range(len(x_text)):\n",
    "        emb = np.zeros(embed_size)\n",
    "        for word in x_text[i]:\n",
    "            try:\n",
    "                emb += embedding_weights[word]\n",
    "            except:\n",
    "                print \"Here\"\n",
    "                pass\n",
    "        emb /= len(x_text[i])\n",
    "        X.append(emb)\n",
    "        y.append(labels[i])\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    return X, y\n",
    "\n",
    "def classification_model(X, Y):\n",
    "    NO_OF_FOLDS=10\n",
    "    X, Y = shuffle(X, Y, random_state=42)\n",
    "    logreg = xgb.XGBClassifier()\n",
    "    scores2 = cross_val_score(logreg, X, Y, cv=NO_OF_FOLDS, scoring='recall_weighted')\n",
    "    print \"Recall(avg): %0.3f (+/- %0.3f)\" % (scores2.mean(), scores2.std() * 2)\n",
    "    scores1 = cross_val_score(logreg, X, Y, cv=NO_OF_FOLDS, scoring='precision_weighted')\n",
    "    print \"Precision(avg): %0.3f (+/- %0.3f)\" % (scores1.mean(), scores1.std() * 2)    \n",
    "    scores3 = cross_val_score(logreg, X, Y, cv=NO_OF_FOLDS, scoring='f1_weighted')\n",
    "    print \"F1-score(avg): %0.3f (+/- %0.3f)\" % (scores3.mean(), scores3.std() * 2)\n",
    "    print(scores1, scores2, scores3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall(avg): 0.860 (+/- 0.032)\n",
      "Precision(avg): 0.861 (+/- 0.032)\n",
      "F1-score(avg): 0.860 (+/- 0.032)\n",
      "(array([ 0.84082122,  0.87697031,  0.85475616,  0.8544143 ,  0.87958115,\n",
      "        0.83578377,  0.8726527 ,  0.8481269 ,  0.86098766,  0.88241741]), array([ 0.84073107,  0.87696335,  0.85340314,  0.85340314,  0.87958115,\n",
      "        0.83507853,  0.87139108,  0.84776903,  0.86089239,  0.88188976]), array([ 0.84072456,  0.87696082,  0.85323414,  0.85332274,  0.87958115,\n",
      "        0.83496655,  0.87129355,  0.84773756,  0.86088664,  0.88185558]))\n"
     ]
    }
   ],
   "source": [
    "X, Y = gen_data(x_text, labels)\n",
    "classification_model(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, trainY = gen_data(X_train, Y_train)\n",
    "testX, testY = gen_data(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: [ 0.59375     0.59473684]\n",
      "\n",
      "Recall: [ 0.59685864  0.59162304]\n",
      "\n",
      "f1_score: [ 0.59530026  0.59317585]\n",
      "\n",
      "[[114  77]\n",
      " [ 78 113]]\n",
      ":: Classification Report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.59      0.60      0.60       191\n",
      "          1       0.59      0.59      0.59       191\n",
      "\n",
      "avg / total       0.59      0.59      0.59       382\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logreg = xgb.XGBClassifier()\n",
    "logreg.fit(trainX, trainY)\n",
    "\n",
    "y_pred = logreg.predict(testX)\n",
    "y_true = Y_test\n",
    "precision = metrics.precision_score(y_true, y_pred, average=None)\n",
    "recall = metrics.recall_score(y_true, y_pred, average=None)\n",
    "f1_score = metrics.f1_score(y_true, y_pred, average=None)\n",
    "print(\"Precision: \" + str(precision) + \"\\n\")\n",
    "print(\"Recall: \" + str(recall) + \"\\n\")\n",
    "print(\"f1_score: \" + str(f1_score) + \"\\n\")\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "print(\":: Classification Report\")\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
