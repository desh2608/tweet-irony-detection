{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import preprocessor as p\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "import preprocessor as p\n",
    "from collections import Counter\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix \n",
    "from tensorflow.contrib import learn\n",
    "from tflearn.data_utils import to_categorical, pad_sequences\n",
    "import os\n",
    "os.environ['KERAS_BACKEND']='theano'\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Merge, Dropout, LSTM, GRU, Bidirectional\n",
    "from keras.models import Model,Sequential\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras import initializers, optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_hashtags = {}\n",
    "def get_hashtags(tweet):\n",
    "    parsed_tweet = p.parse(tweet.decode('ascii', 'ignore').encode('ascii').lower())\n",
    "    parsed_hashtags = parsed_tweet.hashtags\n",
    "    \n",
    "    hashtags = []\n",
    "    if parsed_hashtags is not None:\n",
    "        for hashtag in parsed_hashtags:\n",
    "            temp = hashtag.match[1:].lower()\n",
    "            if temp in all_hashtags:\n",
    "                all_hashtags[temp] += 1\n",
    "            else:\n",
    "                all_hashtags[temp] = 1\n",
    "            hashtags.append(temp)\n",
    "\n",
    "    hashtags_str = (\" \").join(hashtags)\n",
    "    return hashtags_str, len(hashtags)\n",
    "\n",
    "def get_clean_tweet(tweet):\n",
    "    p.set_options(p.OPT.URL)\n",
    "    clean_tweet = p.clean(tweet)\n",
    "    return clean_tweet.lower().replace(\"#\",\" \")\n",
    "\n",
    "\n",
    "emotion_keys = {}\n",
    "def get_emotion(tweet):\n",
    "    result = re.findall(r\":\\w+_\\w+:\",tweet)\n",
    "    if result is not None:\n",
    "        emotions = []\n",
    "        for i in range(len(result)):\n",
    "            emotion = result[i][1:-1]\n",
    "            emotions.append(emotion)\n",
    "            if emotion in emotion_keys:\n",
    "                emotion_keys[emotion] += 1\n",
    "            else:\n",
    "                emotion_keys[emotion] = 1\n",
    "    return (\" \").join(emotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./datasets/train/SemEval2018-T3-train-taskA.txt\", sep=\"\\t\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['hashtags'], data['length'] = zip(*data['Tweet text'].map(get_hashtags)) \n",
    "data[\"tweet\"] = data['Tweet text'].map(get_clean_tweet)\n",
    "data['emotion'] = data['tweet'].map(get_emotion)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_text = data[\"tweet\"].tolist()\n",
    "labels =  data[\"Label\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "f = open(\"./datasets/SARC/data.pkl\", \"r\")\n",
    "x_text = pickle.load(f)\n",
    "label = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_embedding_weights(filename, sep):\n",
    "    embed_dict = {}\n",
    "    file = open(filename,'r')\n",
    "    for line in file.readlines():\n",
    "        row = line.strip().split(sep)\n",
    "        embed_dict[row[0]] = row[1:]\n",
    "    print('Loaded from file: ' + str(filename))\n",
    "    file.close()\n",
    "    return embed_dict\n",
    "\n",
    "def map_embedding_weights(embed, vocab, embed_size):\n",
    "    vocab_size = len(vocab)\n",
    "    embeddingWeights = np.zeros((vocab_size , embed_size))\n",
    "    n = 0\n",
    "    words_missed = []\n",
    "    for k, v in vocab.iteritems():\n",
    "        try:\n",
    "            embeddingWeights[v] = embed[k]\n",
    "        except:\n",
    "            n += 1\n",
    "            words_missed.append(k)\n",
    "            pass\n",
    "    print(\"%d embedding missed\"%n, \" of \" , vocab_size)\n",
    "    return embeddingWeights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AttLayer(Layer):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Create a trainable weight variable for this layer.\n",
    "        self.W = self.add_weight(name='kernel', \n",
    "                                      shape=(input_shape[-1],),\n",
    "                                      initializer='random_normal',\n",
    "                                      trainable=True)\n",
    "        super(AttLayer, self).build(input_shape)  # Be sure to call this somewhere!\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        eij = K.tanh(K.dot(x, self.W))\n",
    "        \n",
    "        ai = K.exp(eij)\n",
    "        weights = ai/K.sum(ai, axis=1).dimshuffle(0,'x')\n",
    "        \n",
    "        weighted_input = x*weights.dimshuffle(0,1,'x')\n",
    "        return weighted_input.sum(axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])\n",
    "\n",
    "def blstm_atten(inp_dim, vocab_size, embed_size, num_classes, learn_rate):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, embed_size, input_length=inp_dim))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Bidirectional(LSTM(embed_size, return_sequences=True)))\n",
    "    model.add(AttLayer())\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "def blstm(inp_dim,vocab_size, embed_size, num_classes, learn_rate):   \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, embed_size, input_length=inp_dim, trainable=True))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Bidirectional(LSTM(embed_size)))\n",
    "    model.add(Dropout(0.50))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_document_length = 50\n",
    "num_classes = 2\n",
    "embed_size = 50\n",
    "n_epoch = 50\n",
    "batch_size = 256\n",
    "learn_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 29106\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(x_text, label, random_state=42, test_size=0.10)\n",
    "\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length, 2)\n",
    "vocab_processor = vocab_processor.fit(x_text)\n",
    "\n",
    "vocab_size = len(vocab_processor.vocabulary_)\n",
    "print(\"Vocabulary Size: {:d}\".format(vocab_size))\n",
    "vocab = vocab_processor.vocabulary_._mapping\n",
    "\n",
    "trainX = np.array(list(vocab_processor.transform(X_train)))\n",
    "testX = np.array(list(vocab_processor.transform(X_test)))\n",
    "\n",
    "trainY = np.asarray(Y_train)\n",
    "testY = np.asarray(Y_test)\n",
    "\n",
    "trainX = pad_sequences(trainX, maxlen=max_document_length, value=0.)\n",
    "testX = pad_sequences(testX, maxlen=max_document_length, value=0.)\n",
    "\n",
    "trainY = to_categorical(trainY, nb_classes=num_classes)\n",
    "testY = to_categorical(testY, nb_classes=num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = blstm_atten(trainX.shape[1], vocab_size, embed_size, num_classes, learn_rate)\n",
    "model.fit(trainX, trainY, epochs=n_epoch, shuffle=True, batch_size=batch_size, validation_split= 0.10,\n",
    "                  verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model(\"sarc_model.h5\", custom_objects={'AttLayer':AttLayer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: [ 0.63329937  0.62662137]\n",
      "\n",
      "Recall: [ 0.62555168  0.63435986]\n",
      "\n",
      "f1_score: [ 0.62940168  0.63046687]\n",
      "\n",
      "[[8079 4836]\n",
      " [4678 8116]]\n",
      ":: Classification Report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.63      0.63      0.63     12915\n",
      "          1       0.63      0.63      0.63     12794\n",
      "\n",
      "avg / total       0.63      0.63      0.63     25709\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temp = model.predict(testX)\n",
    "y_pred  = np.argmax(temp, 1)\n",
    "y_true = np.argmax(testY, 1)\n",
    "precision = metrics.precision_score(y_true, y_pred, average=None)\n",
    "recall = metrics.recall_score(y_true, y_pred, average=None)\n",
    "f1_score = metrics.f1_score(y_true, y_pred, average=None)\n",
    "print(\"Precision: \" + str(precision) + \"\\n\")\n",
    "print(\"Recall: \" + str(recall) + \"\\n\")\n",
    "print(\"f1_score: \" + str(f1_score) + \"\\n\")\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "print(\":: Classification Report\")\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save(\"sarc_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_with_labels(low_dim_embs, labels, filename='tsne.png'):\n",
    "    assert low_dim_embs.shape[0] >= len(labels), 'More labels than embeddings'\n",
    "    plt.figure(figsize=(14, 14))  # in inches\n",
    "    for i, label in enumerate(labels):\n",
    "        x, y = low_dim_embs[i, :]\n",
    "        plt.scatter(x, y)\n",
    "        plt.annotate(label,\n",
    "                     xy=(x, y),\n",
    "                     xytext=(5, 2),\n",
    "                     textcoords='offset points',\n",
    "                     ha='right',\n",
    "                     va='bottom')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_embedding(embedding, reverse_dictionary):\n",
    "    tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=2000)\n",
    "    low_dim_embs = tsne.fit_transform(embedding)\n",
    "    labels = [reverse_dictionary[i] for i in xrange(vocab_size)]\n",
    "    plot_with_labels(low_dim_embs, labels)\n",
    "    return low_dim_embs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reverse_dict = vocab_processor.vocabulary_._reverse_mapping\n",
    "embedding = model.layers[0].get_weights()[0]\n",
    "low_dim_embs = plot_embedding(embedding, reverse_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.utils import shuffle\n",
    "from string import punctuation\n",
    "import re\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import make_scorer, f1_score, accuracy_score, recall_score, precision_score, classification_report, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_data(x_text, labels):\n",
    "    x_text = np.array(list(vocab_processor.transform(x_text)))\n",
    "    embedding_weights = model.layers[0].get_weights()[0]\n",
    "    X, y = [], []\n",
    "    for i in range(len(x_text)):\n",
    "        emb = np.zeros(embed_size)\n",
    "        for word in x_text[i]:\n",
    "            try:\n",
    "                emb += embedding_weights[word]\n",
    "            except:\n",
    "                print \"Here\"\n",
    "                pass\n",
    "        emb /= len(x_text[i])\n",
    "        X.append(emb)\n",
    "        y.append(labels[i])\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    return X, y\n",
    "\n",
    "def classification_model(X, Y):\n",
    "    NO_OF_FOLDS=10\n",
    "    X, Y = shuffle(X, Y, random_state=42)\n",
    "    logreg = xgb.XGBClassifier()\n",
    "    scores2 = cross_val_score(logreg, X, Y, cv=NO_OF_FOLDS, scoring='recall_weighted')\n",
    "    print \"Recall(avg): %0.3f (+/- %0.3f)\" % (scores2.mean(), scores2.std() * 2)\n",
    "    scores1 = cross_val_score(logreg, X, Y, cv=NO_OF_FOLDS, scoring='precision_weighted')\n",
    "    print \"Precision(avg): %0.3f (+/- %0.3f)\" % (scores1.mean(), scores1.std() * 2)    \n",
    "    scores3 = cross_val_score(logreg, X, Y, cv=NO_OF_FOLDS, scoring='f1_weighted')\n",
    "    print \"F1-score(avg): %0.3f (+/- %0.3f)\" % (scores3.mean(), scores3.std() * 2)\n",
    "    print(scores1, scores2, scores3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainX, trainY = gen_data(X_train, Y_train)\n",
    "testX, testY = gen_data(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logreg = xgb.XGBClassifier()\n",
    "logreg.fit(trainX, trainY)\n",
    "\n",
    "y_pred = logreg.predict(testX)\n",
    "y_true = Y_test\n",
    "precision = metrics.precision_score(y_true, y_pred, average=None)\n",
    "recall = metrics.recall_score(y_true, y_pred, average=None)\n",
    "f1_score = metrics.f1_score(y_true, y_pred, average=None)\n",
    "print(\"Precision: \" + str(precision) + \"\\n\")\n",
    "print(\"Recall: \" + str(recall) + \"\\n\")\n",
    "print(\"f1_score: \" + str(f1_score) + \"\\n\")\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "print(\":: Classification Report\")\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, Y = gen_data(x_text, labels)\n",
    "classification_model(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
