{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import wordsegment as ws\n",
    "import pickle\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_progress(progress):\n",
    "    barLength = 100 # Modify this to change the length of the progress bar\n",
    "    status = \"\"\n",
    "    if isinstance(progress, int):\n",
    "        progress = float(progress)\n",
    "    if not isinstance(progress, float):\n",
    "        progress = 0\n",
    "        status = \"error: progress var must be float\\r\\n\"\n",
    "    if progress < 0:\n",
    "        progress = 0\n",
    "        status = \"Halt...\\r\\n\"\n",
    "    if progress >= 1:\n",
    "        progress = 1\n",
    "        status = \"Done...\\r\\n\"\n",
    "    block = int(round(barLength*progress))\n",
    "    text = \"\\rPercent: [{0}] {1}% {2}\".format( \"#\"*block + \"-\"*(barLength-block), progress*100, status)\n",
    "    sys.stdout.write(text)\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading data for processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_train = open(\"./datasets/train/SemEval2018-T3-train-taskA_emoji.txt\",'r',encoding=\"utf-8\")\n",
    "ws.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in below cell is function for obtaining text from a single tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataFromTweet(sample):\n",
    "    idx, label, tweet = sample.split('\\t')\n",
    "    hashtags = [i[1:] for i in tweet.split() if i.startswith(\"#\")]\n",
    "    hashtag_words = []\n",
    "    text_words = [re.sub(r'[^\\w\\s]','',i).lower() for i in tweet.split() if not i.startswith(\"#\")]\n",
    "    for hashtag in hashtags:\n",
    "        hashtag_words.extend(ws.segment(hashtag))\n",
    "    return idx, label, tweet, text_words, hashtag_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function reads all data from file and stores in lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readData(fp):\n",
    "    samples = fp.read().strip().split('\\n')\n",
    "    samples = samples[1:]\n",
    "    idxs = []\n",
    "    tweets = []\n",
    "    tweet_text = []\n",
    "    tweet_hashtags = []\n",
    "    labels = []\n",
    "\n",
    "    for sample in samples:\n",
    "        idx, label, tweet, text_words, hashtag_words = getDataFromTweet(sample)\n",
    "        idxs.append(idx)\n",
    "        labels.append(label)\n",
    "        tweets.append(tweet)\n",
    "        tweet_text.append(text_words)\n",
    "        tweet_hashtags.append(hashtag_words)\n",
    "    \n",
    "    return idxs, tweets, tweet_text, tweet_hashtags, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs, tweets, tweet_text, tweet_hashtags, labels = readData(fp_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3834\n"
     ]
    }
   ],
   "source": [
    "print(len(tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeWordList(sent_list):\n",
    "    wf = {}\n",
    "    for sent in sent_list:\n",
    "        for w in sent:\n",
    "            if w in wf:\n",
    "                wf[w] += 1\n",
    "            else:\n",
    "                wf[w] = 0\n",
    "    wl = {}\n",
    "    rwl = {}\n",
    "    i = 0\n",
    "    for w,f in wf.items():\n",
    "        wl[w] = i\n",
    "        rwl[i] = w\n",
    "        i += 1\n",
    "    wl['UNK'] = i\n",
    "    return wl,rwl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list, rev_word_list = makeWordList(tweet_text+tweet_hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12348\n"
     ]
    }
   ],
   "source": [
    "print(len(word_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapWordToId(sent_contents,word_dict):\n",
    "    T = []\n",
    "    for sent in sent_contents:\n",
    "        t = []\n",
    "        for w in sent:\n",
    "            if w in word_dict:\n",
    "                t.append(word_dict[w])\n",
    "            else:\n",
    "                t.append(word_dict['UNK'])\n",
    "        T.append(t)\n",
    "    return T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_text_id = mapWordToId(tweet_text,word_list)\n",
    "tweet_hashtags_id = mapWordToId(tweet_hashtags,word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1113, 5109, 2293, 10210, 3897, 1508, 2138, 98, 7623, 9741]\n",
      "[8484, 5321, 6401, 2457]\n"
     ]
    }
   ],
   "source": [
    "print (tweet_text_id[0])\n",
    "print (tweet_hashtags_id[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readWordEmb(word_dict, fname, embSize=100):\n",
    "    print (\"Reading word vectors from file\")\n",
    "    wv = []\n",
    "    wl = []\n",
    "    num_lines = sum(1 for line in open(fname, encoding=\"utf-8\"))\n",
    "    with open(fname, 'r',encoding=\"utf-8\") as f:\n",
    "        i = 1\n",
    "        for line in f :\n",
    "            vs = line.split()\n",
    "            if len(vs) < 50 :\n",
    "                continue\n",
    "            vect = map(float, vs[1:])\n",
    "            wv.append(vect)\n",
    "            wl.append(vs[0])\n",
    "            update_progress(i/num_lines)\n",
    "            i += 1\n",
    "    wordemb = []\n",
    "    count = 0\n",
    "    print (\"Reading words from word list\")\n",
    "    for word, id in word_dict.items():\n",
    "        i = 1\n",
    "        if str(word) in wl:\n",
    "            wordemb.append(wv[wl.index(str(word))])\n",
    "        else:\n",
    "            count += 1\n",
    "            wordemb.append(np.random.rand(embSize))\n",
    "        update_progress(i/len(word_dict))\n",
    "        i += 1\n",
    "    wordemb = np.asarray(wordemb, dtype='float32')\n",
    "    print (\"Number of unknown word in word embedding\", count)\n",
    "    return wordemb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading word vectors from file\n",
      "Percent: [###################################################-------------------------------------------------] 51.113099636870615%     "
     ]
    }
   ],
   "source": [
    "emb_file = \"./glove/glove.twitter.27B.100d.txt\"\n",
    "wv = readWordEmb(word_list,emb_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save all data in pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_text = np.array(tweet_text_id)\n",
    "W_hashtags = np.array(tweet_hashtags_id)\n",
    "Y = np.zeros(len(labels), 2)\n",
    "for i in range(len(labels)):\n",
    "    Y[i][labels[i]] = 1\n",
    "\n",
    "with open('./datasets/taskA-train.pickle', 'wb') as handle:\n",
    "    pickle.dump(W_text, handle)\n",
    "    pickle.dump(W_hashtags, handle)\n",
    "    pickle.dump(Y, handle)\n",
    "    pickle.dump(wv, handle)\n",
    "    pickle.dump(word_list, handle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
